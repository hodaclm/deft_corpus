{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro2AI-Lecture6-NLP.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT3dnp1IkNR0"
      },
      "source": [
        "# Intro2AI: Lecture 6 - Introduction to Natural Language Processing\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1jiB_rcx8777OqnMHcll2jYJQZF-sIaPm)\n",
        "\n",
        "\n",
        "In this practical session, we will see how to:\n",
        "- Pre-process data using Spacy\n",
        "- Build a sentiment analysis system, using Scikit-Learn\n",
        "- Generate word embeddings, using Gensim\n",
        "- Advanced exercise: build a neural-based sentiment classifier \n",
        "\n",
        "\n",
        "**Advanced exercises** are meant to be done at home, or at the end of the session if you have time.\n",
        "\n",
        "Our corpus for sentiment classification will be the Pop-corn dataset ( https://www.kaggle.com/ymanojkumar023/kumarmanoj-bag-of-words-meets-bags-of-popcorn/code). \n",
        "\n",
        "\n",
        "We make available a reduced and cleaned version of the dataset in a google drive, the following cells will download the data (we also make available the original dataset that will not be used during this practical session).\n",
        "\n",
        "\n",
        "Side note: too see the pictures, you need to allow cookies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLwHu5tY8LGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c810ddd8-549d-4610-92b4-551c315dcb73"
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=948a7116036f2170a49814b06030be985374e26efc9d3b71ea2e7cd6c55225a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9zNG9vz3gN8"
      },
      "source": [
        "# Downloading data\n",
        "# wiki_ai.txt\n",
        "#wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1dayJql47Thz8dmOF1txyQj77FoktWtGo' -O wiki_ai.txt\n",
        "\n",
        "import wget\n",
        "# wiki_ai.txt (for Part 1)\n",
        "url = \"https://docs.google.com/uc?export=download&id=1dayJql47Thz8dmOF1txyQj77FoktWtGo\"\n",
        "filename = wget.download(url)\n",
        "\n",
        "# Training data (for Part 2)\n",
        "url = \"https://docs.google.com/uc?export=download&id=1VcPE4bo8ygubyLmxwA-jycckUtVcC6l6\"\n",
        "filename = wget.download(url)\n",
        "\n",
        "# Test data (for Part 2)\n",
        "url = \"https://docs.google.com/uc?export=download&id=1GQf17s5Tf7rXobjhDON9gek2gOHyJ4-K\"\n",
        "filename = wget.download(url)\n",
        "\n",
        "# Full dataset (for Part 3)\n",
        "# tokenized, sentence, lower casing and some normlization\n",
        "url = \"https://docs.google.com/uc?export=download&id=1TokJd_dnYksfHjCqMpKEqrLHDhkwSFyV\"\n",
        "filename = wget.download(url)\n",
        "\n",
        "# Original dataset (fyi, not used in this Practical Session)\n",
        "#url = \"https://docs.google.com/uc?export=download&id=1HFGLcWDn_vcmze-L_0jzB40ybmnD2_c1\"\n",
        "#filename = wget.download(url)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEoTsGiehLLa"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Part 1: Using Spacy for data preprocessing\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1aWjN2Fn1g2HYG6_Q9f-9j0JCUjxRJiUM)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Within a computer, text is encoded as a string of characters. \n",
        "In order to analyze text data within NLP applications, we need to properly preprocess it. \n",
        "An NLP preprocessing pipeline generally consists of the following steps :\n",
        "* sentence segmentation\n",
        "* tokenisation\n",
        "* normalization: lower-casing, lemmatization, removing stop-words\n",
        "* pos-tagging\n",
        "* named entity recognition\n",
        "* parsing\n",
        "\n",
        "The first two steps are necessary, while the others are optional.\n",
        "\n",
        "For these exercises, we will use the module **spacy** (already installed on google colab).\n",
        "At the end of this notebook, you can also see how to perform pre-processing using NLTK.\n",
        "\n",
        "Spacy is a python module that implements an NLP pipeline, in order to carry out tasks such as segmentation, tokenization, lemmatization and pos-tagging. \n",
        "We will use it in order to preprocess a document in English.\n",
        "\n",
        "Doc: https://spacy.io/usage/processing-pipelines\n",
        "\n",
        "\n",
        "The text comes from Wikipedia: https://www.wikiwand.com/en/Artificial_intelligence\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubFamr89vq_Z"
      },
      "source": [
        "with open( 'wiki_ai.txt') as infile:\n",
        "  text = infile.read()\n",
        "\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUnGCI8Ad8r3"
      },
      "source": [
        "## 1.1 Tokenisation\n",
        "\n",
        "Spacy can be used to directly tokenize any text. \n",
        "To make it work, you need to load a model specific to the target language, here 'en' for English (there are also some domain specific models).\n",
        "\n",
        "```\n",
        "nlp = spacy.load('en_core_web_sm', entity=True)\n",
        "```\n",
        "\n",
        "This model corresponds to a processing 'pipeline': \n",
        "  - by default, it includes the tokenisation, the lemmatization and the POS tagging\n",
        "  - here, for example, we say that we also want our pipeline to include a model for Named Entity Recognition ('entity = True')\n",
        "\n",
        "Using spacy (see code below):\n",
        "- import the spacy module into Python \n",
        "- load all the necessary models for English\n",
        "- open the file 'wiki_ai.txt' for reading\n",
        "- process it using spacy’s nlp pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q1L4ApskpmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b29b7e84-a768-4e30-b327-b80dc83263f3"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', entity=True)\n",
        "\n",
        "# Read in string of characters\n",
        "with open('wiki_ai.txt') as inFile:\n",
        "    text = inFile.read()\n",
        "\n",
        "# Preprocess using spacy's pipeline\n",
        "doc = nlp(text)\n",
        "\n",
        "print('Preprocessing done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3FdmUgnw5Ox"
      },
      "source": [
        "Our preprocessed document is now present as a list of tokens in our doc variable, and we can access its different annotations by looping through it:\n",
        "- print each individual token, together with its lemmatized form and part of speech tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxMTGR8sw_9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09147f33-9abd-4fc9-cdd7-bbd892c01820"
      },
      "source": [
        "# Inspect tokens, lemmas, and pos tags\n",
        "for token in doc:\n",
        "  print( token.text, token.lemma_, token.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial artificial ADJ\n",
            "intelligence intelligence NOUN\n",
            "( ( PUNCT\n",
            "AI AI PROPN\n",
            ") ) PUNCT\n",
            "is be AUX\n",
            "intelligence intelligence NOUN\n",
            "demonstrated demonstrate VERB\n",
            "by by ADP\n",
            "machines machine NOUN\n",
            ", , PUNCT\n",
            "unlike unlike ADP\n",
            "the the DET\n",
            "natural natural ADJ\n",
            "intelligence intelligence NOUN\n",
            "displayed display VERB\n",
            "by by ADP\n",
            "humans human NOUN\n",
            "and and CCONJ\n",
            "animals animal NOUN\n",
            ", , PUNCT\n",
            "which which DET\n",
            "involves involve VERB\n",
            "consciousness consciousness NOUN\n",
            "and and CCONJ\n",
            "emotionality emotionality NOUN\n",
            ". . PUNCT\n",
            "The the DET\n",
            "distinction distinction NOUN\n",
            "between between ADP\n",
            "the the DET\n",
            "former former ADJ\n",
            "and and CCONJ\n",
            "the the DET\n",
            "latter latter ADJ\n",
            "categories category NOUN\n",
            "is be AUX\n",
            "often often ADV\n",
            "revealed reveal VERB\n",
            "by by ADP\n",
            "the the DET\n",
            "acronym acronym NOUN\n",
            "chosen choose VERB\n",
            ". . PUNCT\n",
            "' ' PUNCT\n",
            "Strong Strong PROPN\n",
            "' ' PUNCT\n",
            "AI ai NOUN\n",
            "is be AUX\n",
            "usually usually ADV\n",
            "labelled label VERB\n",
            "as as SCONJ\n",
            "AGI AGI PROPN\n",
            "( ( PUNCT\n",
            "Artificial Artificial PROPN\n",
            "General General PROPN\n",
            "Intelligence Intelligence PROPN\n",
            ") ) PUNCT\n",
            "while while SCONJ\n",
            "attempts attempt NOUN\n",
            "to to PART\n",
            "emulate emulate VERB\n",
            "' ' PUNCT\n",
            "natural natural ADJ\n",
            "' ' PUNCT\n",
            "intelligence intelligence NOUN\n",
            "have have AUX\n",
            "been be AUX\n",
            "called call VERB\n",
            "ABI ABI PROPN\n",
            "( ( PUNCT\n",
            "Artificial Artificial PROPN\n",
            "Biological Biological PROPN\n",
            "Intelligence Intelligence PROPN\n",
            ") ) PUNCT\n",
            ". . PUNCT\n",
            "\n",
            "\n",
            " \n",
            "\n",
            " SPACE\n",
            "The the DET\n",
            "study study NOUN\n",
            "of of ADP\n",
            "mechanical mechanical ADJ\n",
            "or or CCONJ\n",
            "\" \" PUNCT\n",
            "formal formal ADJ\n",
            "\" \" PUNCT\n",
            "reasoning reasoning NOUN\n",
            "began begin VERB\n",
            "with with ADP\n",
            "philosophers philosopher NOUN\n",
            "and and CCONJ\n",
            "mathematicians mathematician NOUN\n",
            "in in ADP\n",
            "antiquity antiquity NOUN\n",
            ". . PUNCT\n",
            "The the DET\n",
            "study study NOUN\n",
            "of of ADP\n",
            "mathematical mathematical ADJ\n",
            "logic logic NOUN\n",
            "led lead VERB\n",
            "directly directly ADV\n",
            "to to ADP\n",
            "Alan Alan PROPN\n",
            "Turing Turing PROPN\n",
            "'s 's PART\n",
            "theory theory NOUN\n",
            "of of ADP\n",
            "computation computation NOUN\n",
            ", , PUNCT\n",
            "which which DET\n",
            "suggested suggest VERB\n",
            "that that SCONJ\n",
            "a a DET\n",
            "machine machine NOUN\n",
            ", , PUNCT\n",
            "by by ADP\n",
            "shuffling shuffle VERB\n",
            "symbols symbol NOUN\n",
            "as as ADV\n",
            "simple simple ADJ\n",
            "as as SCONJ\n",
            "\" \" PUNCT\n",
            "0 0 NUM\n",
            "\" \" PUNCT\n",
            "and and CCONJ\n",
            "\" \" PUNCT\n",
            "1 1 NUM\n",
            "\" \" PUNCT\n",
            ", , PUNCT\n",
            "could could VERB\n",
            "simulate simulate VERB\n",
            "any any DET\n",
            "conceivable conceivable ADJ\n",
            "act act NOUN\n",
            "of of ADP\n",
            "mathematical mathematical ADJ\n",
            "deduction deduction NOUN\n",
            ". . PUNCT\n",
            "This this DET\n",
            "insight insight NOUN\n",
            ", , PUNCT\n",
            "that that SCONJ\n",
            "digital digital ADJ\n",
            "computers computer NOUN\n",
            "can can VERB\n",
            "simulate simulate VERB\n",
            "any any DET\n",
            "process process NOUN\n",
            "of of ADP\n",
            "formal formal ADJ\n",
            "reasoning reasoning NOUN\n",
            ", , PUNCT\n",
            "is be AUX\n",
            "known know VERB\n",
            "as as SCONJ\n",
            "the the DET\n",
            "Church Church PROPN\n",
            "– – PUNCT\n",
            "Turing ture VERB\n",
            "thesis.[39 thesis.[39 X\n",
            "] ] PUNCT\n",
            "Along along ADP\n",
            "with with ADP\n",
            "concurrent concurrent ADJ\n",
            "discoveries discovery NOUN\n",
            "in in ADP\n",
            "neurobiology neurobiology NOUN\n",
            ", , PUNCT\n",
            "information information NOUN\n",
            "theory theory NOUN\n",
            "and and CCONJ\n",
            "cybernetics cybernetic NOUN\n",
            ", , PUNCT\n",
            "this this DET\n",
            "led lead VERB\n",
            "researchers researcher NOUN\n",
            "to to PART\n",
            "consider consider VERB\n",
            "the the DET\n",
            "possibility possibility NOUN\n",
            "of of ADP\n",
            "building build VERB\n",
            "an an DET\n",
            "electronic electronic ADJ\n",
            "brain brain NOUN\n",
            ". . PUNCT\n",
            "Turing ture VERB\n",
            "proposed propose VERB\n",
            "changing change VERB\n",
            "the the DET\n",
            "question question NOUN\n",
            "from from ADP\n",
            "whether whether SCONJ\n",
            "a a DET\n",
            "machine machine NOUN\n",
            "was be AUX\n",
            "intelligent intelligent ADJ\n",
            ", , PUNCT\n",
            "to to ADP\n",
            "\" \" PUNCT\n",
            "whether whether SCONJ\n",
            "or or CCONJ\n",
            "not not PART\n",
            "it -PRON- PRON\n",
            "is be AUX\n",
            "possible possible ADJ\n",
            "for for ADP\n",
            "machinery machinery NOUN\n",
            "to to PART\n",
            "show show VERB\n",
            "intelligent intelligent ADJ\n",
            "behaviour\".[40 behaviour\".[40 NOUN\n",
            "] ] PUNCT\n",
            "The the DET\n",
            "first first ADJ\n",
            "work work NOUN\n",
            "that that DET\n",
            "is be AUX\n",
            "now now ADV\n",
            "generally generally ADV\n",
            "recognized recognize VERB\n",
            "as as SCONJ\n",
            "AI AI PROPN\n",
            "was be AUX\n",
            "McCullouch McCullouch PROPN\n",
            "and and CCONJ\n",
            "Pitts Pitts PROPN\n",
            "' ' PART\n",
            "1943 1943 NUM\n",
            "formal formal ADJ\n",
            "design design NOUN\n",
            "for for ADP\n",
            "Turing turing NOUN\n",
            "- - PUNCT\n",
            "complete complete ADJ\n",
            "\" \" PUNCT\n",
            "artificial artificial ADJ\n",
            "neurons\".[41 neurons\".[41 NOUN\n",
            "] ] PUNCT\n",
            "\n",
            "\n",
            " \n",
            "\n",
            " SPACE\n",
            "The the DET\n",
            "field field NOUN\n",
            "of of ADP\n",
            "AI AI PROPN\n",
            "research research NOUN\n",
            "was be AUX\n",
            "born bear VERB\n",
            "at at ADP\n",
            "a a DET\n",
            "workshop workshop NOUN\n",
            "at at ADP\n",
            "Dartmouth Dartmouth PROPN\n",
            "College College PROPN\n",
            "in in ADP\n",
            "1956,[42 1956,[42 NUM\n",
            "] ] PUNCT\n",
            "where where ADV\n",
            "the the DET\n",
            "term term NOUN\n",
            "\" \" PUNCT\n",
            "Artificial Artificial PROPN\n",
            "Intelligence Intelligence PROPN\n",
            "\" \" PUNCT\n",
            "was be AUX\n",
            "coined coin VERB\n",
            "by by ADP\n",
            "John John PROPN\n",
            "McCarthy McCarthy PROPN\n",
            "to to PART\n",
            "distinguish distinguish VERB\n",
            "the the DET\n",
            "field field NOUN\n",
            "from from ADP\n",
            "cybernetics cybernetic NOUN\n",
            "and and CCONJ\n",
            "escape escape VERB\n",
            "the the DET\n",
            "influence influence NOUN\n",
            "of of ADP\n",
            "the the DET\n",
            "cyberneticist cyberneticist NOUN\n",
            "Norbert Norbert PROPN\n",
            "Wiener.[43 Wiener.[43 PROPN\n",
            "] ] PUNCT\n",
            "Attendees Attendees PROPN\n",
            "Allen Allen PROPN\n",
            "Newell Newell PROPN\n",
            "( ( PUNCT\n",
            "CMU CMU PROPN\n",
            ") ) PUNCT\n",
            ", , PUNCT\n",
            "Herbert Herbert PROPN\n",
            "Simon Simon PROPN\n",
            "( ( PUNCT\n",
            "CMU CMU PROPN\n",
            ") ) PUNCT\n",
            ", , PUNCT\n",
            "John John PROPN\n",
            "McCarthy McCarthy PROPN\n",
            "( ( PUNCT\n",
            "MIT MIT PROPN\n",
            ") ) PUNCT\n",
            ", , PUNCT\n",
            "Marvin Marvin PROPN\n",
            "Minsky Minsky PROPN\n",
            "( ( PUNCT\n",
            "MIT MIT PROPN\n",
            ") ) PUNCT\n",
            "and and CCONJ\n",
            "Arthur Arthur PROPN\n",
            "Samuel Samuel PROPN\n",
            "( ( PUNCT\n",
            "IBM IBM PROPN\n",
            ") ) PUNCT\n",
            "became become VERB\n",
            "the the DET\n",
            "founders founder NOUN\n",
            "and and CCONJ\n",
            "leaders leader NOUN\n",
            "of of ADP\n",
            "AI AI PROPN\n",
            "research.[44 research.[44 VERB\n",
            "] ] PUNCT\n",
            "They -PRON- PRON\n",
            "and and CCONJ\n",
            "their -PRON- DET\n",
            "students student NOUN\n",
            "produced produce VERB\n",
            "programs program NOUN\n",
            "that that DET\n",
            "the the DET\n",
            "press press NOUN\n",
            "described describe VERB\n",
            "as as SCONJ\n",
            "\" \" PUNCT\n",
            "astonishing\":[45 astonishing\":[45 ADJ\n",
            "] ] PUNCT\n",
            "computers computer NOUN\n",
            "were be AUX\n",
            "learning learn VERB\n",
            "checkers checker NOUN\n",
            "strategies strategy NOUN\n",
            "( ( PUNCT\n",
            "c. c. PROPN\n",
            "1954)[46 1954)[46 PROPN\n",
            "] ] PUNCT\n",
            "( ( PUNCT\n",
            "and and CCONJ\n",
            "by by ADP\n",
            "1959 1959 NUM\n",
            "were be AUX\n",
            "reportedly reportedly ADV\n",
            "playing play VERB\n",
            "better better ADV\n",
            "than than SCONJ\n",
            "the the DET\n",
            "average average ADJ\n",
            "human),[47 human),[47 ADV\n",
            "] ] PUNCT\n",
            "solving solve VERB\n",
            "word word NOUN\n",
            "problems problem NOUN\n",
            "in in ADP\n",
            "algebra algebra NOUN\n",
            ", , PUNCT\n",
            "proving prove VERB\n",
            "logical logical ADJ\n",
            "theorems theorem NOUN\n",
            "( ( PUNCT\n",
            "Logic Logic PROPN\n",
            "Theorist Theorist PROPN\n",
            ", , PUNCT\n",
            "first first ADV\n",
            "run run NOUN\n",
            "c. c. PROPN\n",
            "1956 1956 NUM\n",
            ") ) PUNCT\n",
            "and and CCONJ\n",
            "speaking speak VERB\n",
            "English.[48 english.[48 NOUN\n",
            "] ] PUNCT\n",
            "By by ADP\n",
            "the the DET\n",
            "middle middle NOUN\n",
            "of of ADP\n",
            "the the DET\n",
            "1960s 1960 NOUN\n",
            ", , PUNCT\n",
            "research research NOUN\n",
            "in in ADP\n",
            "the the DET\n",
            "U.S. U.S. PROPN\n",
            "was be AUX\n",
            "heavily heavily ADV\n",
            "funded fund VERB\n",
            "by by ADP\n",
            "the the DET\n",
            "Department Department PROPN\n",
            "of of ADP\n",
            "Defense[49 Defense[49 PROPN\n",
            "] ] PUNCT\n",
            "and and CCONJ\n",
            "laboratories laboratory NOUN\n",
            "had have AUX\n",
            "been be AUX\n",
            "established establish VERB\n",
            "around around ADP\n",
            "the the DET\n",
            "world.[50 world.[50 NOUN\n",
            "] ] PUNCT\n",
            "AI AI PROPN\n",
            "'s 's PART\n",
            "founders founder NOUN\n",
            "were be AUX\n",
            "optimistic optimistic ADJ\n",
            "about about ADP\n",
            "the the DET\n",
            "future future NOUN\n",
            ": : PUNCT\n",
            "Herbert Herbert PROPN\n",
            "Simon Simon PROPN\n",
            "predicted predict VERB\n",
            ", , PUNCT\n",
            "\" \" PUNCT\n",
            "machines machine NOUN\n",
            "will will VERB\n",
            "be be AUX\n",
            "capable capable ADJ\n",
            ", , PUNCT\n",
            "within within ADP\n",
            "twenty twenty NUM\n",
            "years year NOUN\n",
            ", , PUNCT\n",
            "of of ADP\n",
            "doing do VERB\n",
            "any any DET\n",
            "work work NOUN\n",
            "a a DET\n",
            "man man NOUN\n",
            "can can VERB\n",
            "do do AUX\n",
            "\" \" PUNCT\n",
            ". . PUNCT\n",
            "Marvin Marvin PROPN\n",
            "Minsky Minsky PROPN\n",
            "agreed agree VERB\n",
            ", , PUNCT\n",
            "writing write VERB\n",
            ", , PUNCT\n",
            "\" \" PUNCT\n",
            "within within ADP\n",
            "a a DET\n",
            "generation generation NOUN\n",
            "... ... PUNCT\n",
            "the the DET\n",
            "problem problem NOUN\n",
            "of of ADP\n",
            "creating create VERB\n",
            "' ' PUNCT\n",
            "artificial artificial ADJ\n",
            "intelligence intelligence NOUN\n",
            "' ' PUNCT\n",
            "will will VERB\n",
            "substantially substantially ADV\n",
            "be be AUX\n",
            "solved\".[13 solved\".[13 X\n",
            "] ] PUNCT\n",
            "\n",
            " \n",
            " SPACE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HcJhX3DfCTm"
      },
      "source": [
        "#### Pandas\n",
        "\n",
        "You can use Pandas to better visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VW_9Glf1Gsz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "546128a5-daac-40a6-87c3-56ec61d0d973"
      },
      "source": [
        "# Using pandas for a better visualization \n",
        "import pandas as pd\n",
        "\n",
        "spacy_pos_tagged = [(w, w.tag_, w.pos_) for w in doc]\n",
        "pd.DataFrame(spacy_pos_tagged,\n",
        "             columns=['Word', 'POS tag', 'Tag type'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Word POS tag Tag type\n",
              "0       Artificial      JJ      ADJ\n",
              "1     intelligence      NN     NOUN\n",
              "2                (   -LRB-    PUNCT\n",
              "3               AI     NNP    PROPN\n",
              "4                )   -RRB-    PUNCT\n",
              "..             ...     ...      ...\n",
              "463  substantially      RB      ADV\n",
              "464             be      VB      AUX\n",
              "465    solved\".[13      FW        X\n",
              "466              ]   -RRB-    PUNCT\n",
              "467             \\n     _SP    SPACE\n",
              "\n",
              "[468 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24a5e552-dd17-433f-82aa-e00ffd132a22\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>POS tag</th>\n",
              "      <th>Tag type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Artificial</td>\n",
              "      <td>JJ</td>\n",
              "      <td>ADJ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>intelligence</td>\n",
              "      <td>NN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(</td>\n",
              "      <td>-LRB-</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>)</td>\n",
              "      <td>-RRB-</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>substantially</td>\n",
              "      <td>RB</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>be</td>\n",
              "      <td>VB</td>\n",
              "      <td>AUX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>solved\".[13</td>\n",
              "      <td>FW</td>\n",
              "      <td>X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>]</td>\n",
              "      <td>-RRB-</td>\n",
              "      <td>PUNCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>\\n</td>\n",
              "      <td>_SP</td>\n",
              "      <td>SPACE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>468 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24a5e552-dd17-433f-82aa-e00ffd132a22')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24a5e552-dd17-433f-82aa-e00ffd132a22 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24a5e552-dd17-433f-82aa-e00ffd132a22');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8EJ7iH2mUo-"
      },
      "source": [
        "#### Look at the results:\n",
        "* lemmatization = base form / remove inflectional part:\n",
        "  - *agents* -> *agent* / *categories* -> *category*\n",
        "  - *achieving* -> *achieve* / *began* -> *begin*\n",
        "  - *been / is / was* -> *be*\n",
        "  - *(* -> *(* PUNCT\n",
        "  - *its* -> *-PRON-*\n",
        "* strange things / not perfect:\n",
        "  - *Turing* -> *ture* VERB / *Turing* -> *turing* NOUN\n",
        "  - *thesis.[39* -> *thesis.[39* :  problem with footnote mentions, after a period, that are not well segmented\n",
        "  - *AI* -> *AI* PROPN vs *AI* -> *ai* NOUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjqXNiCOnvlz"
      },
      "source": [
        "#### Notes on POS tags:\n",
        "\n",
        "* You can use the method 'explain' to have information about some annotation, for example the POS tags, see the code below.\n",
        "* Here we used a very small set of POS (vs e.g. 36 in the PTB: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXStTWgunYqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433b751f-9973-4f1f-8a74-15eb90cc0849"
      },
      "source": [
        "# Inspect POS tags\n",
        "all_tags = set()\n",
        "for token in doc:\n",
        "  all_tags.add(token.pos_)\n",
        "for tag in all_tags:\n",
        "  print( tag, spacy.explain(tag)) # explain each label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PUNCT punctuation\n",
            "NUM numeral\n",
            "PRON pronoun\n",
            "PART particle\n",
            "CCONJ coordinating conjunction\n",
            "SPACE space\n",
            "PROPN proper noun\n",
            "X other\n",
            "ADP adposition\n",
            "SCONJ subordinating conjunction\n",
            "ADJ adjective\n",
            "NOUN noun\n",
            "ADV adverb\n",
            "VERB verb\n",
            "DET determiner\n",
            "AUX auxiliary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dC9lMGoZIc"
      },
      "source": [
        "## 1.2 Segmenting into sentences\n",
        "\n",
        "Apart from token segmentation, Spacy has also automatically segmented our document intro sentences. Print out the different sentences of the document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQmuXqxuoyVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d8c8ed-0d8e-4d0c-bb4f-5d20d9e5559e"
      },
      "source": [
        "# Print the sentences\n",
        "for i, sent in enumerate( doc.sents ):\n",
        "  print( i, sent.text.strip() )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality.\n",
            "1 The distinction between the former and the latter categories is often revealed by the acronym chosen. '\n",
            "2 Strong' AI is usually labelled as AGI (Artificial General Intelligence) while attempts to emulate 'natural' intelligence have been called ABI (Artificial Biological Intelligence).\n",
            "3 The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity.\n",
            "4 The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction.\n",
            "5 This insight, that digital computers can simulate any process of formal reasoning, is known as the Church–Turing thesis.[39]\n",
            "6 Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain.\n",
            "7 Turing proposed changing the question from whether a machine was intelligent, to \"whether or not it is possible for machinery to show intelligent behaviour\".[40]\n",
            "8 The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".[41]\n",
            "9 The field of AI research was born at a workshop at Dartmouth College in 1956,[42] where the term \"Artificial Intelligence\" was coined by John McCarthy to distinguish the field from cybernetics and escape the influence of the cyberneticist\n",
            "10 Norbert Wiener.[43]\n",
            "11 Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research.[44]\n",
            "12 They and their students produced programs that the press described as \"astonishing\":[45] computers were learning checkers strategies (c. 1954)[46] (and by 1959 were reportedly playing better than the average human),[47] solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956) and speaking English.[48] By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[49] and laboratories had been established around the world.[50]\n",
            "13 AI's founders were optimistic about the future: Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".\n",
            "14 Marvin Minsky agreed, writing, \"within a generation ...\n",
            "15 the problem of creating 'artificial intelligence' will substantially be solved\".[13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMVp0BF3pGdl"
      },
      "source": [
        "#### Note on sentence segmentation \n",
        "* an apostroph is not well segmented around *Strong*, it appears at the end of sentence 1 instead of the beginning of sentence 2.\n",
        "* Sentence 10: ???\n",
        "* Sentence 12: should be segmented \"and speaking English.[48] By the middle of the 1960s,\"\n",
        "* Sentence 14-15: problem with a '...' that shouldn't be segmented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-1IyOOTpkML"
      },
      "source": [
        "## 1.3 Named entity recognition\n",
        "\n",
        "As part of the preprocessing pipeline, Spacy has equally carried out named entity recognition.\n",
        "* print out each named entity, together with the label assigned to it\n",
        "* what do the labels stand for?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOu1lpzop19t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d496ef0c-5c72-432e-efe4-a21b0f23a9a5"
      },
      "source": [
        "entity_labels = set()\n",
        "for entity in doc.ents:\n",
        "  label = entity.label_\n",
        "  print( entity.text, '\\t', label )\n",
        "  entity_labels.add( label )\n",
        "\n",
        "print( '\\nEntity labels:' )\n",
        "for l in entity_labels:\n",
        "  print( l, spacy.explain(l))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial General Intelligence \t ORG\n",
            "ABI \t ORG\n",
            "Artificial Biological Intelligence \t ORG\n",
            "Alan Turing's \t PERSON\n",
            "Church \t ORG\n",
            "first \t ORDINAL\n",
            "McCullouch \t PERSON\n",
            "Pitts' 1943 \t ORG\n",
            "Dartmouth College \t ORG\n",
            "1956,[42 \t CARDINAL\n",
            "John McCarthy \t PERSON\n",
            "Norbert Wiener.[43 \t EVENT\n",
            "Allen Newell \t PERSON\n",
            "Herbert Simon \t PERSON\n",
            "John McCarthy \t PERSON\n",
            "MIT \t ORG\n",
            "Marvin Minsky \t PERSON\n",
            "MIT \t ORG\n",
            "Arthur Samuel \t PERSON\n",
            "IBM \t ORG\n",
            "1959 \t DATE\n",
            "algebra \t GPE\n",
            "Logic Theorist \t ORG\n",
            "1956 \t DATE\n",
            "the middle of the 1960s \t DATE\n",
            "U.S. \t GPE\n",
            "the Department of Defense[49] \t ORG\n",
            "Herbert Simon \t PERSON\n",
            "twenty years \t DATE\n",
            "Marvin Minsky \t PERSON\n",
            "\n",
            "Entity labels:\n",
            "EVENT Named hurricanes, battles, wars, sports events, etc.\n",
            "ORDINAL \"first\", \"second\", etc.\n",
            "PERSON People, including fictional\n",
            "GPE Countries, cities, states\n",
            "ORG Companies, agencies, institutions, etc.\n",
            "DATE Absolute or relative dates or periods\n",
            "CARDINAL Numerals that do not fall under another type\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPGZqq3sfyNm"
      },
      "source": [
        "#### Visualization\n",
        "\n",
        "A module called 'displacy' can be used to visualize the Named Entities directly in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U04k54xT0ORj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "outputId": "5a345324-78d3-4535-e8e3-c03acb6e1ddf"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# Visually\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality. The distinction between the former and the latter categories is often revealed by the acronym chosen. 'Strong' AI is usually labelled as AGI (\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Artificial General Intelligence\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ") while attempts to emulate 'natural' intelligence have been called \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ABI\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Artificial Biological Intelligence\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "). </br></br>The study of mechanical or &quot;formal&quot; reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Alan Turing's\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " theory of computation, which suggested that a machine, by shuffling symbols as simple as &quot;0&quot; and &quot;1&quot;, could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Church\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "–Turing thesis.[39] Along with concurrent discoveries in neurobiology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. Turing proposed changing the question from whether a machine was intelligent, to &quot;whether or not it is possible for machinery to show intelligent behaviour&quot;.[40] The \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " work that is now generally recognized as AI was \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    McCullouch\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Pitts' 1943\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " formal design for Turing-complete &quot;artificial neurons&quot;.[41]</br></br>The field of AI research was born at a workshop at \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Dartmouth College\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1956,[42\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              "] where the term &quot;Artificial Intelligence&quot; was coined by \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    John McCarthy\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " to distinguish the field from cybernetics and escape the influence of the cyberneticist \n",
              "<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Norbert Wiener.[43\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">EVENT</span>\n",
              "</mark>\n",
              "] Attendees \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Allen Newell\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (CMU), \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Herbert Simon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (CMU), \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    John McCarthy\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    MIT\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "), \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Marvin Minsky\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    MIT\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ") and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Arthur Samuel\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    IBM\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ") became the founders and leaders of AI research.[44] They and their students produced programs that the press described as &quot;astonishing&quot;:[45] computers were learning checkers strategies (c. 1954)[46] (and by \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1959\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " were reportedly playing better than the average human),[47] solving word problems in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    algebra\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", proving logical theorems (\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Logic Theorist\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", first run c. \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1956\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ") and speaking English.[48] By \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the middle of the 1960s\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", research in the \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.S.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " was heavily funded by \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Department of Defense[49]\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " and laboratories had been established around the world.[50] AI's founders were optimistic about the future: \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Herbert Simon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " predicted, &quot;machines will be capable, within \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    twenty years\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", of doing any work a man can do&quot;. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Marvin Minsky\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " agreed, writing, &quot;within a generation ... the problem of creating 'artificial intelligence' will substantially be solved&quot;.[13]\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkJAPYKCxyrd"
      },
      "source": [
        "Note on Named Entity Recognition\n",
        "* \"Church\" = organization instead or person"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q6a-haVzKTS"
      },
      "source": [
        "## 1.4 Syntactic parsing\n",
        "\n",
        "Syntactic parsers produce an analysis of the sentences, where the words are connected to each other through syntactic relations.\n",
        "We can easily parse sentences with Spacy, in order to produce a dependency graph over the sentences. \n",
        "The dependency relations can be used as features for other systems, to know who did what, or to know which word is modified by an adjective.\n",
        "\n",
        "More info: https://spacy.io/usage/linguistic-features#dependency-parse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZE9gtNAzOT4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "0df663f7-0e6d-4baf-f0fd-58b9e259ee6b"
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "example_sentence = \"You can make dependency trees.\"\n",
        "example_doc = nlp(example_sentence)\n",
        "\n",
        "# Visualization\n",
        "displacy.render(example_doc, style=\"dep\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3446115ce01847d6b57bd1b5d4ad7981-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">You</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">can</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">make</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">dependency</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">trees.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3446115ce01847d6b57bd1b5d4ad7981-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3446115ce01847d6b57bd1b5d4ad7981-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3446115ce01847d6b57bd1b5d4ad7981-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3446115ce01847d6b57bd1b5d4ad7981-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3446115ce01847d6b57bd1b5d4ad7981-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3446115ce01847d6b57bd1b5d4ad7981-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-3446115ce01847d6b57bd1b5d4ad7981-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-3446115ce01847d6b57bd1b5d4ad7981-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NZMWtkYzT0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "d03cddfe-274b-437b-b8f4-450264c0e333"
      },
      "source": [
        "# Print the first sentence of our document\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(sentences[0])\n",
        "doc = nlp(sentences[0])\n",
        "\n",
        "# Visualization\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"be00813ba47c44b3a4e5e2d4aa861a95-0\" class=\"displacy\" width=\"3900\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Artificial</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">intelligence (</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">AI)</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">intelligence</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">demonstrated</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">by</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">machines,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">unlike</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">natural</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">intelligence</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">displayed</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">by</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">humans</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">animals,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">which</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">involves</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">consciousness</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">emotionality.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,264.5 560.0,264.5 560.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-2\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M380.0,441.5 L388.0,429.5 372.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-3\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M730.0,441.5 L738.0,429.5 722.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-4\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M905.0,441.5 L913.0,429.5 897.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-5\" stroke-width=\"2px\" d=\"M945,439.5 C945,352.0 1080.0,352.0 1080.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">agent</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1080.0,441.5 L1088.0,429.5 1072.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-6\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,352.0 1255.0,352.0 1255.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1255.0,441.5 L1263.0,429.5 1247.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-7\" stroke-width=\"2px\" d=\"M595,439.5 C595,89.5 1445.0,89.5 1445.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1445.0,441.5 L1453.0,429.5 1437.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-8\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,264.5 1960.0,264.5 1960.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1645,441.5 L1637,429.5 1653,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-9\" stroke-width=\"2px\" d=\"M1820,439.5 C1820,352.0 1955.0,352.0 1955.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1820,441.5 L1812,429.5 1828,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-10\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,177.0 1965.0,177.0 1965.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1965.0,441.5 L1973.0,429.5 1957.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-11\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,352.0 2130.0,352.0 2130.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2130.0,441.5 L2138.0,429.5 2122.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-12\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,352.0 2305.0,352.0 2305.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">agent</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2305.0,441.5 L2313.0,429.5 2297.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-13\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,352.0 2480.0,352.0 2480.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2480.0,441.5 L2488.0,429.5 2472.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-14\" stroke-width=\"2px\" d=\"M2520,439.5 C2520,352.0 2655.0,352.0 2655.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2655.0,441.5 L2663.0,429.5 2647.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-15\" stroke-width=\"2px\" d=\"M2520,439.5 C2520,264.5 2835.0,264.5 2835.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2835.0,441.5 L2843.0,429.5 2827.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-16\" stroke-width=\"2px\" d=\"M3045,439.5 C3045,352.0 3180.0,352.0 3180.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3045,441.5 L3037,429.5 3053,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-17\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,2.0 3200.0,2.0 3200.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3200.0,441.5 L3208.0,429.5 3192.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-18\" stroke-width=\"2px\" d=\"M3220,439.5 C3220,352.0 3355.0,352.0 3355.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3355.0,441.5 L3363.0,429.5 3347.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-19\" stroke-width=\"2px\" d=\"M3395,439.5 C3395,352.0 3530.0,352.0 3530.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3530.0,441.5 L3538.0,429.5 3522.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-20\" stroke-width=\"2px\" d=\"M3395,439.5 C3395,264.5 3710.0,264.5 3710.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-be00813ba47c44b3a4e5e2d4aa861a95-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3710.0,441.5 L3718.0,429.5 3702.0,429.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFXSO1GDgQJY"
      },
      "source": [
        "#### Navigating the parse tree\n",
        "\n",
        "Each element of the tree is associated to attributes: you can use them to inspect the different elements of the trees.\n",
        "\n",
        "See below a tabular version of the tree where each token id associated to its head, with the relation ('amod') between them. The eventual children of the current token are also printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB2ceahj2ivz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "outputId": "503e7b65-fc23-45d3-dcf2-df9b6c616b56"
      },
      "source": [
        "# Navigating the parse tree\n",
        "spacy_dep_rel = [(w.text, w.dep_, w.head.text, w.head.pos_, [child.text for child in w.children]) for w in doc]\n",
        "pd.DataFrame(spacy_dep_rel,\n",
        "             columns=['Word', 'Dep', 'Head text', 'Head pos', 'children'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Word    Dep      Head text Head pos  \\\n",
              "0      Artificial   amod   intelligence     NOUN   \n",
              "1    intelligence  nsubj             is      AUX   \n",
              "2               (  punct   intelligence     NOUN   \n",
              "3              AI  appos   intelligence     NOUN   \n",
              "4               )  punct   intelligence     NOUN   \n",
              "5              is   ROOT             is      AUX   \n",
              "6    intelligence   attr             is      AUX   \n",
              "7    demonstrated    acl   intelligence     NOUN   \n",
              "8              by  agent   demonstrated     VERB   \n",
              "9        machines   pobj             by      ADP   \n",
              "10              ,  punct             is      AUX   \n",
              "11         unlike   prep             is      AUX   \n",
              "12            the    det   intelligence     NOUN   \n",
              "13        natural   amod   intelligence     NOUN   \n",
              "14   intelligence   pobj         unlike      ADP   \n",
              "15      displayed    acl   intelligence     NOUN   \n",
              "16             by  agent      displayed     VERB   \n",
              "17         humans   pobj             by      ADP   \n",
              "18            and     cc         humans     NOUN   \n",
              "19        animals   conj         humans     NOUN   \n",
              "20              ,  punct   intelligence     NOUN   \n",
              "21          which  nsubj       involves     VERB   \n",
              "22       involves  relcl   intelligence     NOUN   \n",
              "23  consciousness   dobj       involves     VERB   \n",
              "24            and     cc  consciousness     NOUN   \n",
              "25   emotionality   conj  consciousness     NOUN   \n",
              "26              .  punct             is      AUX   \n",
              "\n",
              "                                      children  \n",
              "0                                           []  \n",
              "1                       [Artificial, (, AI, )]  \n",
              "2                                           []  \n",
              "3                                           []  \n",
              "4                                           []  \n",
              "5   [intelligence, intelligence, ,, unlike, .]  \n",
              "6                               [demonstrated]  \n",
              "7                                         [by]  \n",
              "8                                   [machines]  \n",
              "9                                           []  \n",
              "10                                          []  \n",
              "11                              [intelligence]  \n",
              "12                                          []  \n",
              "13                                          []  \n",
              "14      [the, natural, displayed, ,, involves]  \n",
              "15                                        [by]  \n",
              "16                                    [humans]  \n",
              "17                              [and, animals]  \n",
              "18                                          []  \n",
              "19                                          []  \n",
              "20                                          []  \n",
              "21                                          []  \n",
              "22                      [which, consciousness]  \n",
              "23                         [and, emotionality]  \n",
              "24                                          []  \n",
              "25                                          []  \n",
              "26                                          []  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ff69b5e-a0db-431e-98a1-ed9c86a2bab9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Dep</th>\n",
              "      <th>Head text</th>\n",
              "      <th>Head pos</th>\n",
              "      <th>children</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Artificial</td>\n",
              "      <td>amod</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>intelligence</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>is</td>\n",
              "      <td>AUX</td>\n",
              "      <td>[Artificial, (, AI, )]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(</td>\n",
              "      <td>punct</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AI</td>\n",
              "      <td>appos</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>)</td>\n",
              "      <td>punct</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>is</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>is</td>\n",
              "      <td>AUX</td>\n",
              "      <td>[intelligence, intelligence, ,, unlike, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>intelligence</td>\n",
              "      <td>attr</td>\n",
              "      <td>is</td>\n",
              "      <td>AUX</td>\n",
              "      <td>[demonstrated]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>demonstrated</td>\n",
              "      <td>acl</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[by]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>by</td>\n",
              "      <td>agent</td>\n",
              "      <td>demonstrated</td>\n",
              "      <td>VERB</td>\n",
              "      <td>[machines]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>machines</td>\n",
              "      <td>pobj</td>\n",
              "      <td>by</td>\n",
              "      <td>ADP</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>,</td>\n",
              "      <td>punct</td>\n",
              "      <td>is</td>\n",
              "      <td>AUX</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>unlike</td>\n",
              "      <td>prep</td>\n",
              "      <td>is</td>\n",
              "      <td>AUX</td>\n",
              "      <td>[intelligence]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>the</td>\n",
              "      <td>det</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>natural</td>\n",
              "      <td>amod</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>intelligence</td>\n",
              "      <td>pobj</td>\n",
              "      <td>unlike</td>\n",
              "      <td>ADP</td>\n",
              "      <td>[the, natural, displayed, ,, involves]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>displayed</td>\n",
              "      <td>acl</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[by]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>by</td>\n",
              "      <td>agent</td>\n",
              "      <td>displayed</td>\n",
              "      <td>VERB</td>\n",
              "      <td>[humans]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>humans</td>\n",
              "      <td>pobj</td>\n",
              "      <td>by</td>\n",
              "      <td>ADP</td>\n",
              "      <td>[and, animals]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>and</td>\n",
              "      <td>cc</td>\n",
              "      <td>humans</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>animals</td>\n",
              "      <td>conj</td>\n",
              "      <td>humans</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>,</td>\n",
              "      <td>punct</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>which</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>involves</td>\n",
              "      <td>VERB</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>involves</td>\n",
              "      <td>relcl</td>\n",
              "      <td>intelligence</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[which, consciousness]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>consciousness</td>\n",
              "      <td>dobj</td>\n",
              "      <td>involves</td>\n",
              "      <td>VERB</td>\n",
              "      <td>[and, emotionality]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>and</td>\n",
              "      <td>cc</td>\n",
              "      <td>consciousness</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>emotionality</td>\n",
              "      <td>conj</td>\n",
              "      <td>consciousness</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>.</td>\n",
              "      <td>punct</td>\n",
              "      <td>is</td>\n",
              "      <td>AUX</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ff69b5e-a0db-431e-98a1-ed9c86a2bab9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ff69b5e-a0db-431e-98a1-ed9c86a2bab9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ff69b5e-a0db-431e-98a1-ed9c86a2bab9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DozsAlqxlIs"
      },
      "source": [
        "### Getting some help\n",
        "\n",
        "Hint: You can either access Spacy's manual on the internet to find out how to access the information, or look at the built-in help by typing *help(doc)*.\n",
        "\n",
        "https://spacy.io/api/doc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPVhl79mxs58"
      },
      "source": [
        "help(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-T2diU3q7B9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Part 2: Sentiment analysis, \"Bag of Words Meets Bags of Popcorn\"\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=13nwT3niIwy8jJKEyTF0dRaHeEi1q8Zlv)\n",
        "\n",
        "In this part, we will make experiments on sentiment analysis on movie reviews.\n",
        "The reviews are either positive (label 1) or negative (label 0).\n",
        "\n",
        "The data come from: https://www.kaggle.com/ymanojkumar023/kumarmanoj-bag-of-words-meets-bags-of-popcorn/code \n",
        "\n",
        "In this part, we will:\n",
        "- vectorize the data using a bag-of-word representation\n",
        "- train and evaluate a classifier for sentiment analysis.\n",
        "\n",
        "To this aim, we will use the scikit-learn library.\n",
        "It is already installed within google colab.\n",
        "\n",
        "\n",
        "Scikit-Learn website: https://scikit-learn.org/stable/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNuTf1mTLYfq"
      },
      "source": [
        "## 2.1 Reading the data\n",
        "\n",
        "The cell below contains the code to read the data and print the first instances.\n",
        "\n",
        "The data have already been tokenized and normalized (i.e. lowercased). \n",
        "Data are balanced: there is an equal number of positive and negative examples in both the training an test set.\n",
        "We have 5000 training instances, and 500 test instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmUjkWJfaNGM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "2a53c96b-1484-4248-b8c1-98c85d78b5da"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Read data using panda\n",
        "import pandas as pd    \n",
        "\n",
        "def read_data( infile ):\n",
        "  data = pd.read_csv(infile, header=0, \\\n",
        "                    delimiter=\"\\t\", quoting=3)\n",
        "  print(\"Number of examples:\", data.shape[0],\"\\n\")\n",
        "\n",
        "  reviews = data[\"review\"]\n",
        "  labels = data[\"sentiment\"]\n",
        "  return data, reviews, labels\n",
        "\n",
        "print( \"\\n-- Reading training data \")\n",
        "train, train_reviews, train_labels =read_data( \"popcorn_clean_train_5000.tsv\" )\n",
        "\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Reading training data \n",
            "Number of examples: 5000 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sentiment                                             review\n",
              "0          0  \"This kind of film has become old hat by now, ...\n",
              "1          0  \"What an appalling piece of rubbish!!! Who ARE...\n",
              "2          0  \"Bloodsuckers has the potential to be a somewh...\n",
              "3          1  \"You do not get more dark or tragic than \\\"Oth...\n",
              "4          1  \"Last night I finished re-watching \\\"Jane Eyre..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6cbd941-c9b7-4fec-aa36-900e0d50eaf5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\"This kind of film has become old hat by now, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>\"What an appalling piece of rubbish!!! Who ARE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>\"Bloodsuckers has the potential to be a somewh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>\"You do not get more dark or tragic than \\\"Oth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>\"Last night I finished re-watching \\\"Jane Eyre...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6cbd941-c9b7-4fec-aa36-900e0d50eaf5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6cbd941-c9b7-4fec-aa36-900e0d50eaf5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6cbd941-c9b7-4fec-aa36-900e0d50eaf5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJxuy14Xtrhr"
      },
      "source": [
        "## 2.2 Feature extraction\n",
        "\n",
        "Now, we are going to transform our text data into vectors. \n",
        "We'll start with simple bag-of-words features. \n",
        "\n",
        "The class **CountVectorizer** implements this transformation:\n",
        "- It converts a collection of text documents to a matrix of token counts (= raw frequency)\n",
        "\n",
        "There are many parameters that can be modified, the main ones are:\n",
        "  - analyzer='word': can be changed to 'char' if you want to use characters as features\n",
        "  - max_features=N: build a vocabulary that only consider the top N features ordered by term frequency across the corpus.\n",
        "\n",
        "To transform your data, you need to:\n",
        "- build a CountVectorizer object with the desired options\n",
        "```\n",
        "vectorizer = CountVectorizer( analyzer = 'word', max_features=1000 )\n",
        "```\n",
        "- learn the transformation on your input data \n",
        "```\n",
        "vectorizer.fit( train_reviews )\n",
        "```\n",
        "- transform your data into the desired output using the learned vectorizer\n",
        "```\n",
        "train_features = vectorizer.transform( train_reviews )\n",
        "```\n",
        "- the method \"fit_transform\" automatically learns AND applies the transformation to the input data.\n",
        "```\n",
        "train_features = vectorizer.fit_transform( train_reviews )\n",
        "```\n",
        "\n",
        "Note that without filtering, this produces **vectors of 39328 dimensions**! \n",
        "Here we arbitrarily reduce to 1000 (but other values should be tested).\n",
        "\n",
        "Many other options are implemented, e.g.:\n",
        "- 'stop_words='english': will automatically remove stop-words from a list (but be careful: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words)\n",
        "- binary (default False): If True, all non zero counts are set to 1. \n",
        "- ngram_range (tuple (min_n, max_n), default=(1, 1)): The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.\n",
        "\n",
        "See the doc: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp7Y0kEt43fV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3591e1f-39fd-470b-e9f1-f8a43c3401ba"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer( analyzer = 'word', max_features=1000 )\n",
        "train_features = vectorizer.fit_transform( train_reviews )\n",
        "\n",
        "print( \"data vectorized\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data vectorized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STgyakb3TQO4"
      },
      "source": [
        "#### Look at the vectorization\n",
        "\n",
        "- Print the shape of the matrix (nb of instances x nb of features)\n",
        "- Print the vocabulary, i.e. the unique words used as features\n",
        "- Print the vector representing the first review\n",
        "- Print the word corresponding to the first non-zero dimension, here dimension 6 (index = 5). Check that it appears once in the first review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtYx-O8-TOFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34df12e8-1559-451c-b8d4-712d5cf15f3e"
      },
      "source": [
        "# array of shape (n_samples, n_features)\n",
        "print( \"Shape of the data, ie nb of examples x number of features:\", train_features.shape )\n",
        "\n",
        "# print the vocabulary (= unique words, here 1000)\n",
        "print( \"\\nVocabulary:\", list( vectorizer.vocabulary_.keys() ) )\n",
        "vocab = vectorizer.get_feature_names()\n",
        "print(  \"Sorted vocabulary:\", vocab )\n",
        "\n",
        "# print the vector representing the first review (1000 dimensions)\n",
        "# use toarray() to densify the matrix --> many 0s = sparsity\n",
        "print( \"\\nVector representing the first review\", train_features[0].toarray())\n",
        "\n",
        "# sixth dimension, value =5\n",
        "# what is the corresponding word?\n",
        "# - invert the dictionnary\n",
        "index_to_token = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
        "print( \"\\nWord corresponding to the 5th dimension:\", index_to_token[5])\n",
        "print( \"First review\", train_reviews[0]) # 2nd sentence: \"some human drama about what could\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the data, ie nb of examples x number of features: (5000, 1000)\n",
            "\n",
            "Vocabulary: ['this', 'kind', 'of', 'film', 'has', 'become', 'old', 'by', 'now', 'it', 'the', 'whole', 'thing', 'is', 'turned', 'in', 'upon', 'itself', 'some', 'br', 'sure', 'sounds', 'like', 'good', 'idea', 'great', 'cast', 'and', 'human', 'drama', 'about', 'what', 'could', 'have', 'might', 'been', 'unfortunately', 'there', 'no', 'that', 'them', 'all', 'together', 'was', 'big', 'one', 'those', 'movies', 'films', 'you', 'end', 'up', 'to', 'see', 'more', 'or', 'two', 'particular', 'people', 'instead', 'getting', 'short', 'takes', 'on', 'everyone', 'not', 'just', 'annoying', 'average', 'script', 'doesn', 'help', 'an', 'piece', 'who', 'are', 'these', 'how', 'yes', 'but', 'enough', 'plot', 'boring', 'reality', 'show', 'so', 'made', 'as', 'characters', 'they', 'don', 'feel', 'for', 'if', 'based', 'real', 'then', 'very', 'sorry', 'violence', 'seems', 'quite', 'being', 'think', 'much', 'him', 'either', 'oh', 'had', 'move', 'potential', 'be', 'somewhat', 'decent', 'movie', 'down', 'space', 'with', 'things', 'even', 'full', 'various', 'different', 'many', 'which', 'yet', 'find', 'out', 'cool', 'well', 'most', 'life', 'outside', 'earth', 'meant', 'work', 'given', 'nature', 'hell', 'almost', 'know', 'comes', 'across', 'low', 'budget', 'action', 'parts', 'pretty', 'particularly', 'gore', 'flick', 'including', 'blood', 'lot', 'character', 'stuff', 'coming', 'from', 'between', 'mostly', 'seem', 'woman', 'few', 'scenes', 'actually', 'actors', 'play', 'too', 'badly', 'nice', 'try', 'at', 'watch', 'called', 'development', 'acting', 'okay', 'michael', 'fun', 'ever', 'least', 'couple', 'women', 'hot', 'aren', 'bad', 'clearly', 'story', 'again', 'said', 'world', 'created', 'little', 'bit', 'way', 'set', 'really', 'does', 'tv', 'series', 'etc', 'head', 'moments', 'makes', 'scene', 'where', 'our', 'talking', 'type', 'hilarious', 'sex', 'indeed', 'though', 'awful', 'usually', 'music', 'playing', 'over', 'half', 'only', 'best', 'basically', 're', 'something', 'cheesy', 'do', 'get', 'dark', 'than', 'fairly', 'performances', 'viewer', 'he', 'falls', 'realistic', 'episode', 'wife', 'becomes', 'against', 'his', 'clear', 'since', 'although', 'usual', 'talent', 'also', 'lines', 'were', 'cut', 'ones', 'any', 'would', 'quickly', 'director', 'performance', 'relationship', 'need', 'further', 'earlier', 'feeling', 'each', 'other', 'sense', 'late', 'into', 'time', 'their', 'hand', 'maybe', 'did', 'meet', 'last', 'night', 'watching', 'jane', 'romance', 'novel', 'classic', 'english', 'my', 'favorite', 'james', 'mr', 'poor', 'year', 'girl', 'years', 'friend', 'gave', 'me', 'words', 'book', 'amazing', 'read', 'times', 'am', 'still', 'its', 'beautiful', 'above', 'main', 'love', 'filmmakers', 'screen', 'imdb', 'famous', 'william', 'george', 'york', 'played', 'society', 'age', 've', 'seen', 'minutes', 'long', 'version', 'plays', 'class', 'powerful', 'strong', 'feels', 'during', 'five', 'wonderful', 'experience', 'every', 'completely', 'successful', 'home', 'alone', 'rock', 'fact', 'worse', 'laugh', 'when', 'credits', 'started', 'will', 'point', 'exactly', 'why', 'first', 'off', 'ridiculous', 'around', 'car', 'want', 'dumb', 'similar', 'rather', 'funny', 'didn', 'once', 'kid', 'comic', 'done', 'bunch', 'single', 'here', 'absolutely', 'while', 'act', 'look', 'street', 'can', 'saying', 'man', 'kids', 'may', 'better', 'before', 'considering', 'reviews', 'money', 'otherwise', 'should', 'reason', 'stars', 'named', 'killer', 'trying', 'kill', 'her', 'doing', 'killing', 'family', 'friends', 'begin', 'let', 'say', 'horror', 'haven', 'scary', 'stupid', 'dialog', 'god', 'girls', 'us', 'japanese', 'review', 'never', 'hear', 'talk', 'guy', 'lots', 'understand', 'seriously', 'ask', 'put', 'list', 'worst', 'without', 'seeing', 'going', 'surprised', 'gets', 'released', 'dvd', 'cheap', 'your', 'dull', 'die', 'star', '10', 'give', 'got', 'interesting', 'figure', 'material', 'actual', 'buy', 'must', 'disappointed', 'romantic', 'everything', 'excellent', 'songs', 'french', 'probably', 'day', 'american', 'hollywood', 'era', 'song', 'dance', 'anything', 'extremely', 'loved', 'especially', 'direction', 'nearly', 'perfect', 'several', 'memorable', 'right', 'after', 'lead', 'stage', 'own', 'situation', 'definitely', 'high', 'points', 'cinema', 'history', 'make', 'anyone', 'same', 'features', 'through', 'television', 'interest', 'directors', 'death', 'please', 'note', 'wrong', 'important', 'line', 'musical', 'brothers', 'making', 'hours', '20', 'found', 'course', 'horrible', 'fine', 'job', 'stand', 'we', 'watched', 'air', 'sister', 'written', 'fans', 'voice', 'isn', 'new', 'change', 'go', 'back', 'true', 'stories', 'general', 'creepy', 'british', 'period', 'style', 'young', 'small', 'near', 'middle', 'guess', 'nothing', 'none', 'later', 'spirit', 'black', 'she', 'evil', 'sees', 'camera', 'reading', 'expect', 'moment', 'goes', 'turn', 'house', 'll', 'elements', 'room', 'somehow', 'someone', 'another', 'genre', 'white', 'female', 'happens', 'ways', 'beginning', 'audience', 'use', 'person', 'events', 'until', 'wanted', 'early', 'view', 'apparently', 'next', 'suspense', 'whether', 'stop', 'production', 'works', 'casting', 'simply', 'joke', 'having', 'looks', 'because', 'supporting', 'town', 'rich', 'premise', 'believable', 'begins', 'take', 'finally', 'fantastic', 'shot', 'career', 'child', 'remember', 'box', 'however', 'truly', 'original', 'within', 'top', 'himself', 'police', 'tells', 'knows', 'brother', 'leave', 'case', 'living', 'father', 'message', 'keep', 'tension', 'killed', 'highly', 'recommend', 'fight', 'matter', 'atmosphere', 'both', 'always', 'gives', 'lost', 'setting', 'light', 'guys', 'often', 'under', 'past', 'boys', '80', 'certain', 'miss', 'perhaps', 'whose', 'state', 'today', 'sets', 'wouldn', 'comment', 'rate', 'theme', 'write', 'waste', 'place', 'start', 'unless', 'dr', 'come', 'meets', 'believe', 'truth', 'writer', 'boy', 'title', 'baby', 'king', 'greatest', 'cannot', 'save', 'actor', 'role', 'looked', 'leaves', 'due', 'away', 'robert', 'group', 'men', 'modern', 'else', 'thought', 'hard', 'odd', 'enjoyed', 'older', 'brilliant', 'comments', 'joe', 'three', 'parents', 'talented', 'self', 'overall', 'serious', 'herself', 'chance', 'unique', 'despite', 'second', 'success', 'singing', 'pure', 'able', 'comedy', 'de', 'la', 'such', 'entertaining', 'hardly', 'rent', 'looking', 'won', 'law', 'doubt', 'supposed', 'typical', 'perfectly', 'dramatic', 'entertainment', 'opening', 'tried', 'flat', 'terrible', 'mother', 'wasn', 'ago', 'already', 'turns', 'mentioned', 'opinion', 'score', 'used', 'couldn', 'silly', 'tell', 'totally', 'humor', 'male', 'ben', 'bring', 'live', 'among', 'directing', 'episodes', 'city', 'part', 'gay', 'tom', 'peter', 'son', 'local', 'daughter', 'call', 'dead', 'major', 'eventually', 'roles', 'screenplay', 'obviously', 'told', 'lives', 'along', 'worth', 'familiar', 'children', 'happy', 'hold', 'future', 'form', 'felt', 'behind', 'final', 'name', 'appears', 'eye', 'feature', 'mind', 'close', 'saw', 'rest', 'hero', 'finds', 'needs', 'happen', 'took', 'nor', 'culture', 'jokes', 'left', 'mean', 'mess', 'sad', 'themselves', 'complete', 'needed', 'hit', 'others', 'taking', 'lack', 'quality', 'follow', 'ending', 'cinematography', 'expected', 'avoid', 'yourself', 'predictable', 'heard', 'david', 'shows', 'dog', 'dialogue', 'means', 'art', 'seemed', 'problems', 'using', 'previous', 'war', 'viewing', 'video', 'game', 'attempt', 'add', 'interested', 'far', 'writing', 'myself', 'sequences', 'heart', 'filmed', 'non', 'side', 'shown', 'knew', 'fire', 'disney', 'cartoon', 'portrayed', 'forced', 'hour', 'became', 'certainly', 'liked', 'wonder', 'fails', 'doctor', 'murder', 'whom', 'effect', 'mystery', 'actress', 'says', 'inside', 'copy', 'number', 'rating', 'cute', 'known', 'leads', 'ok', 'viewers', 'hands', 'poorly', 'happened', 'thinking', 'days', 'straight', 'wish', 'hope', 'brought', 'simple', 'power', 'giving', 'tale', 'shots', 'leading', 'sexual', 'fast', 'possible', 'political', 'mark', 'starts', 'total', 'wait', 'third', 'problem', 'picture', 'kept', 'went', 'deal', 'beyond', 'sit', 'attempts', 'hate', 'care', 'ends', 'lady', 'body', 'realize', 'minute', 'plus', 'crap', 'run', 'oscar', 'huge', 'monster', 'except', 'crazy', 'surprise', 'moving', 'taken', 'husband', 'towards', 'directed', 'richard', 'wants', 'tries', 'match', 'anyway', 'red', 'john', 'eyes', 'enjoy', 'editing', 'question', 'level', '30', 'fall', 'ten', 'strange', 'sometimes', 'crew', 'somewhere', 'gone', 'uses', 'effects', 'possibly', 'whatever', 'superb', 'less', 'working', 'co', 'sequence', 'animation', 'enjoyable', 'zombie', 'theater', 'check', 'release', 'sound', 'imagine', 'laughs', 'lame', 'acted', 'america', 'effort', 'keeps', 'sort', 'storyline', 'thriller', 'nudity', 'secret', 'deserves', 'sequel', 'forget', 'easy', 'came', 'decided', 'entire', 'business', 'attention', 'free', 'weak', 'involved', 'positive', 'soon', 'documentary', 'slow', 'subject', 'writers', 'throughout', 'shame', 'missing', 'face', 'special', 'crime', 'country', 'bill', 'word', 'incredible', 'season', 'easily', 'jack', 'present', 'cop', 'stewart', 'fan', 'water', 'difficult', 'obvious', 'open', 'dream', 'learn', 'school', 'stay', 'paul', 'result', 'mention', 'science', 'fantasy', 'weird', 'battle', 'team', 'biggest', 'running', 'four', 'beauty', 'background', 'emotional', 'showing', 'plenty', 'apart', 'soundtrack', 'order', 'lee', 'example', 'footage']\n",
            "Sorted vocabulary: ['10', '20', '30', '80', 'able', 'about', 'above', 'absolutely', 'across', 'act', 'acted', 'acting', 'action', 'actor', 'actors', 'actress', 'actual', 'actually', 'add', 'after', 'again', 'against', 'age', 'ago', 'air', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'amazing', 'america', 'american', 'among', 'an', 'and', 'animation', 'annoying', 'another', 'any', 'anyone', 'anything', 'anyway', 'apart', 'apparently', 'appears', 'are', 'aren', 'around', 'art', 'as', 'ask', 'at', 'atmosphere', 'attempt', 'attempts', 'attention', 'audience', 'average', 'avoid', 'away', 'awful', 'baby', 'back', 'background', 'bad', 'badly', 'based', 'basically', 'battle', 'be', 'beautiful', 'beauty', 'became', 'because', 'become', 'becomes', 'been', 'before', 'begin', 'beginning', 'begins', 'behind', 'being', 'believable', 'believe', 'ben', 'best', 'better', 'between', 'beyond', 'big', 'biggest', 'bill', 'bit', 'black', 'blood', 'body', 'book', 'boring', 'both', 'box', 'boy', 'boys', 'br', 'brilliant', 'bring', 'british', 'brother', 'brothers', 'brought', 'budget', 'bunch', 'business', 'but', 'buy', 'by', 'call', 'called', 'came', 'camera', 'can', 'cannot', 'car', 'care', 'career', 'cartoon', 'case', 'cast', 'casting', 'certain', 'certainly', 'chance', 'change', 'character', 'characters', 'cheap', 'check', 'cheesy', 'child', 'children', 'cinema', 'cinematography', 'city', 'class', 'classic', 'clear', 'clearly', 'close', 'co', 'come', 'comedy', 'comes', 'comic', 'coming', 'comment', 'comments', 'complete', 'completely', 'considering', 'cool', 'cop', 'copy', 'could', 'couldn', 'country', 'couple', 'course', 'crap', 'crazy', 'created', 'credits', 'creepy', 'crew', 'crime', 'culture', 'cut', 'cute', 'dance', 'dark', 'daughter', 'david', 'day', 'days', 'de', 'dead', 'deal', 'death', 'decent', 'decided', 'definitely', 'deserves', 'despite', 'development', 'dialog', 'dialogue', 'did', 'didn', 'die', 'different', 'difficult', 'directed', 'directing', 'direction', 'director', 'directors', 'disappointed', 'disney', 'do', 'doctor', 'documentary', 'does', 'doesn', 'dog', 'doing', 'don', 'done', 'doubt', 'down', 'dr', 'drama', 'dramatic', 'dream', 'due', 'dull', 'dumb', 'during', 'dvd', 'each', 'earlier', 'early', 'earth', 'easily', 'easy', 'editing', 'effect', 'effects', 'effort', 'either', 'elements', 'else', 'emotional', 'end', 'ending', 'ends', 'english', 'enjoy', 'enjoyable', 'enjoyed', 'enough', 'entertaining', 'entertainment', 'entire', 'episode', 'episodes', 'era', 'especially', 'etc', 'even', 'events', 'eventually', 'ever', 'every', 'everyone', 'everything', 'evil', 'exactly', 'example', 'excellent', 'except', 'expect', 'expected', 'experience', 'extremely', 'eye', 'eyes', 'face', 'fact', 'fails', 'fairly', 'fall', 'falls', 'familiar', 'family', 'famous', 'fan', 'fans', 'fantastic', 'fantasy', 'far', 'fast', 'father', 'favorite', 'feature', 'features', 'feel', 'feeling', 'feels', 'felt', 'female', 'few', 'fight', 'figure', 'film', 'filmed', 'filmmakers', 'films', 'final', 'finally', 'find', 'finds', 'fine', 'fire', 'first', 'five', 'flat', 'flick', 'follow', 'footage', 'for', 'forced', 'forget', 'form', 'found', 'four', 'free', 'french', 'friend', 'friends', 'from', 'full', 'fun', 'funny', 'further', 'future', 'game', 'gave', 'gay', 'general', 'genre', 'george', 'get', 'gets', 'getting', 'girl', 'girls', 'give', 'given', 'gives', 'giving', 'go', 'god', 'goes', 'going', 'gone', 'good', 'gore', 'got', 'great', 'greatest', 'group', 'guess', 'guy', 'guys', 'had', 'half', 'hand', 'hands', 'happen', 'happened', 'happens', 'happy', 'hard', 'hardly', 'has', 'hate', 'have', 'haven', 'having', 'he', 'head', 'hear', 'heard', 'heart', 'hell', 'help', 'her', 'here', 'hero', 'herself', 'high', 'highly', 'hilarious', 'him', 'himself', 'his', 'history', 'hit', 'hold', 'hollywood', 'home', 'hope', 'horrible', 'horror', 'hot', 'hour', 'hours', 'house', 'how', 'however', 'huge', 'human', 'humor', 'husband', 'idea', 'if', 'imagine', 'imdb', 'important', 'in', 'including', 'incredible', 'indeed', 'inside', 'instead', 'interest', 'interested', 'interesting', 'into', 'involved', 'is', 'isn', 'it', 'its', 'itself', 'jack', 'james', 'jane', 'japanese', 'job', 'joe', 'john', 'joke', 'jokes', 'just', 'keep', 'keeps', 'kept', 'kid', 'kids', 'kill', 'killed', 'killer', 'killing', 'kind', 'king', 'knew', 'know', 'known', 'knows', 'la', 'lack', 'lady', 'lame', 'last', 'late', 'later', 'laugh', 'laughs', 'law', 'lead', 'leading', 'leads', 'learn', 'least', 'leave', 'leaves', 'lee', 'left', 'less', 'let', 'level', 'life', 'light', 'like', 'liked', 'line', 'lines', 'list', 'little', 'live', 'lives', 'living', 'll', 'local', 'long', 'look', 'looked', 'looking', 'looks', 'lost', 'lot', 'lots', 'love', 'loved', 'low', 'made', 'main', 'major', 'make', 'makes', 'making', 'male', 'man', 'many', 'mark', 'match', 'material', 'matter', 'may', 'maybe', 'me', 'mean', 'means', 'meant', 'meet', 'meets', 'memorable', 'men', 'mention', 'mentioned', 'mess', 'message', 'michael', 'middle', 'might', 'mind', 'minute', 'minutes', 'miss', 'missing', 'modern', 'moment', 'moments', 'money', 'monster', 'more', 'most', 'mostly', 'mother', 'move', 'movie', 'movies', 'moving', 'mr', 'much', 'murder', 'music', 'musical', 'must', 'my', 'myself', 'mystery', 'name', 'named', 'nature', 'near', 'nearly', 'need', 'needed', 'needs', 'never', 'new', 'next', 'nice', 'night', 'no', 'non', 'none', 'nor', 'not', 'note', 'nothing', 'novel', 'now', 'nudity', 'number', 'obvious', 'obviously', 'odd', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'older', 'on', 'once', 'one', 'ones', 'only', 'open', 'opening', 'opinion', 'or', 'order', 'original', 'oscar', 'other', 'others', 'otherwise', 'our', 'out', 'outside', 'over', 'overall', 'own', 'parents', 'part', 'particular', 'particularly', 'parts', 'past', 'paul', 'people', 'perfect', 'perfectly', 'performance', 'performances', 'perhaps', 'period', 'person', 'peter', 'picture', 'piece', 'place', 'play', 'played', 'playing', 'plays', 'please', 'plenty', 'plot', 'plus', 'point', 'points', 'police', 'political', 'poor', 'poorly', 'portrayed', 'positive', 'possible', 'possibly', 'potential', 'power', 'powerful', 'predictable', 'premise', 'present', 'pretty', 'previous', 'probably', 'problem', 'problems', 'production', 'pure', 'put', 'quality', 'question', 'quickly', 'quite', 'rate', 'rather', 'rating', 're', 'read', 'reading', 'real', 'realistic', 'reality', 'realize', 'really', 'reason', 'recommend', 'red', 'relationship', 'release', 'released', 'remember', 'rent', 'rest', 'result', 'review', 'reviews', 'rich', 'richard', 'ridiculous', 'right', 'robert', 'rock', 'role', 'roles', 'romance', 'romantic', 'room', 'run', 'running', 'sad', 'said', 'same', 'save', 'saw', 'say', 'saying', 'says', 'scary', 'scene', 'scenes', 'school', 'science', 'score', 'screen', 'screenplay', 'script', 'season', 'second', 'secret', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'sees', 'self', 'sense', 'sequel', 'sequence', 'sequences', 'series', 'serious', 'seriously', 'set', 'sets', 'setting', 'several', 'sex', 'sexual', 'shame', 'she', 'short', 'shot', 'shots', 'should', 'show', 'showing', 'shown', 'shows', 'side', 'silly', 'similar', 'simple', 'simply', 'since', 'singing', 'single', 'sister', 'sit', 'situation', 'slow', 'small', 'so', 'society', 'some', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'songs', 'soon', 'sorry', 'sort', 'sound', 'sounds', 'soundtrack', 'space', 'special', 'spirit', 'stage', 'stand', 'star', 'stars', 'start', 'started', 'starts', 'state', 'stay', 'stewart', 'still', 'stop', 'stories', 'story', 'storyline', 'straight', 'strange', 'street', 'strong', 'stuff', 'stupid', 'style', 'subject', 'success', 'successful', 'such', 'superb', 'supporting', 'supposed', 'sure', 'surprise', 'surprised', 'suspense', 'take', 'taken', 'takes', 'taking', 'tale', 'talent', 'talented', 'talk', 'talking', 'team', 'television', 'tell', 'tells', 'ten', 'tension', 'terrible', 'than', 'that', 'the', 'theater', 'their', 'them', 'theme', 'themselves', 'then', 'there', 'these', 'they', 'thing', 'things', 'think', 'thinking', 'third', 'this', 'those', 'though', 'thought', 'three', 'thriller', 'through', 'throughout', 'time', 'times', 'title', 'to', 'today', 'together', 'told', 'tom', 'too', 'took', 'top', 'total', 'totally', 'towards', 'town', 'tried', 'tries', 'true', 'truly', 'truth', 'try', 'trying', 'turn', 'turned', 'turns', 'tv', 'two', 'type', 'typical', 'under', 'understand', 'unfortunately', 'unique', 'unless', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'using', 'usual', 'usually', 'various', 've', 'version', 'very', 'video', 'view', 'viewer', 'viewers', 'viewing', 'violence', 'voice', 'wait', 'want', 'wanted', 'wants', 'war', 'was', 'wasn', 'waste', 'watch', 'watched', 'watching', 'water', 'way', 'ways', 'we', 'weak', 'weird', 'well', 'went', 'were', 'what', 'whatever', 'when', 'where', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whom', 'whose', 'why', 'wife', 'will', 'william', 'wish', 'with', 'within', 'without', 'woman', 'women', 'won', 'wonder', 'wonderful', 'word', 'words', 'work', 'working', 'works', 'world', 'worse', 'worst', 'worth', 'would', 'wouldn', 'write', 'writer', 'writers', 'writing', 'written', 'wrong', 'year', 'years', 'yes', 'yet', 'york', 'you', 'young', 'your', 'yourself', 'zombie']\n",
            "\n",
            "Vector representing the first review [[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 3 0 0 0 0 1 0 0 0\n",
            "  0 0 3 0 3 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 5 0 0 0 0 0 1 0 1 0 3 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4 0 0 1 0 0 0 2 0 0 1 0 0 0\n",
            "  0 3 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            "  0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]]\n",
            "\n",
            "Word corresponding to the 5th dimension: about\n",
            "First review \"This kind of film has become old hat by now, hasn't it? The whole thing is syrupy nostalgia turned in upon itself in some kind of feedback loop.<br /><br />It sure sounds like a good idea: a great ensemble cast, some good gags, and some human drama about what could have/might have been. Unfortunately, there is no central event that binds them all together, like there was in \\\"The Big Chill\\\", one of those seminal movies that spawned copycat films like this one. You end up wanting to see more of one or two particular people instead of getting short takes on everyone. The superficiality this creates is not just annoying, it's maddening. The below-average script doesn't help.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pDzO4QLO5br"
      },
      "source": [
        "## 2.3 Preparing test data\n",
        "\n",
        "We also need to pre-process and vectorize the test set.\n",
        "\n",
        "The difference is:\n",
        "- the **vectorization is 'learned' on the training data** only, on the test set, we use only the 'transform' method of the vectorizer (without the 'fit' part): words that do not appear in our training set are considered 'unknown'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPg0TnCNPPgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e0e534-aaa1-49c0-f8d6-643b921f0c8c"
      },
      "source": [
        "print( \"-- Reading test data \")\n",
        "test, test_reviews, test_labels = read_data( \"popcorn_clean_test_500.tsv\" )\n",
        "\n",
        "test_features = vectorizer.transform( test_reviews )\n",
        "print( \"Vectorized, shape:\", test_features.shape )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Reading test data \n",
            "Number of examples: 500 \n",
            "\n",
            "Vectorized, shape: (500, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK9SoDbUzo_c"
      },
      "source": [
        "## 2.4 Classification without neurons: Scikit-Learn\n",
        "\n",
        "Now we can train a model and use to make predictions on our test set.\n",
        "- Choose an algorithm, e.g. LogisticRegression (aka MaxEnt)\n",
        "- Train on the training set \n",
        "- Make predictions on the development set\n",
        "- Report performance by comparing the gold labels from the evaluation set (i.e. test_labels') to the predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gClrPWfVjEp_"
      },
      "source": [
        "#### First: training step\n",
        "\n",
        "We make the import corresponding to the classifier we want to use (here, Logistic Regression), and train the classifier on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiaxzD8u426g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6bd566-1cfa-4013-d64f-fe9305807539"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit( train_features, train_labels )\n",
        "print( 'Training done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4V_WSfmVGCd"
      },
      "source": [
        "#### Second: predictions\n",
        "\n",
        "Use the model learned to make predictions on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plc6V1FCVKhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c028e916-e6a6-4d87-c7bc-1dffb8b6e26f"
      },
      "source": [
        "preds = classifier.predict( test_features )\n",
        "print( \"Predictions done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr_bWy1hVJHo"
      },
      "source": [
        "#### Finally: scores\n",
        "\n",
        "Scoring is done by comparing the gold labels (i.e. the ones annotated by an human) to the predicted labels assigned by the model.\n",
        "\n",
        "Scikit-learn provides a method called \"classification_report\" that gives an overview of the performance using different metrics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djLyDS2IVUCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e87b00-3360-40d8-a5f0-54e9653a79dd"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print( classification_report( test_labels, preds ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       250\n",
            "           1       0.86      0.82      0.84       250\n",
            "\n",
            "    accuracy                           0.84       500\n",
            "   macro avg       0.84      0.84      0.84       500\n",
            "weighted avg       0.84      0.84      0.84       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijw_d2PRledM"
      },
      "source": [
        "#### Notes on the results\n",
        "\n",
        "The simplest metrics is the accuracy: it corresponds to the fraction of examples correctly labeled over the total number of examples.\n",
        "\n",
        "Other metrics are given, expecially per label metrics: the F1 for each class is an indication of the performance of your model per class.\n",
        "\n",
        "For more information about the metrics, look at scikit documentation.\n",
        "\n",
        "The performance of this model are rather good: it correctly identifies 84% of the test examples.\n",
        "This task is rather simple, since the performance are good while we have a small training set and a very simple data representation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG2WhhvRok7F"
      },
      "source": [
        "## 2.5 Improving the results\n",
        "\n",
        "Many parts of a model can be modified to try to improve the performance:\n",
        "- the data representation\n",
        "- the values of the hyper-parameters\n",
        "- the choice of the algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI5uoQXpdL2s"
      },
      "source": [
        "### 2.5.1 Modifying data representation \n",
        "Data representation corresponds to the choice of features.\n",
        "Here, we choosed a simple bag-of-word representation (BOW) with raw frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU7EZMS9pxBq"
      },
      "source": [
        "#### 2.5.1.1 TF-IDF normalization\n",
        "\n",
        "As said during the course, BOW comes with many flavors, and a good option in general is to use TF-IDF normalization instead of raw features.\n",
        "\n",
        "With scikit, you can either directly vectorize using TF-IDF (with the class 'TfidfVectorizer') or transform a count-based representation (with the class 'TfidfTransformer'). \n",
        "Here, we use this second option.\n",
        "\n",
        "We then train and evaluate our model again, with this new representation.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUXVnDJcp8sI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf69091-4e16-497e-f7d5-ecb649caaaef"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer()\n",
        "train_features_tfidf = transformer.fit_transform(train_features)\n",
        "test_features_tfidf = transformer.transform(test_features)\n",
        "\n",
        "# Training\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit( train_features_tfidf, train_labels )\n",
        "# Predictions\n",
        "preds = classifier.predict( test_features_tfidf )\n",
        "# Scores\n",
        "print( classification_report( test_labels, preds ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       250\n",
            "           1       0.87      0.84      0.86       250\n",
            "\n",
            "    accuracy                           0.86       500\n",
            "   macro avg       0.86      0.86      0.86       500\n",
            "weighted avg       0.86      0.86      0.86       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDtSeIuQp9K_"
      },
      "source": [
        "#### 2.5.1.2 Testing n-grams features\n",
        "\n",
        " As said during the course, BOW doesn't take into account the context of each word, which can be crucial for the task.\n",
        "\n",
        " Let's try with n-grams, remind that: \n",
        " - unigrams: single tokens, same as BOW\n",
        " - bigrams: two words\n",
        " - trigrams: three words\n",
        "\n",
        " Here we're going to test bi-grams, tri-grams and a concatenation of unigrams, bi-grams and tri-grams.\n",
        " This is done with the option 'ngram_range'.\n",
        "\n",
        " The code to test bi-grams is given below, you have to write a similar code to test tri-grams and the concatenation.\n",
        "\n",
        " Note that here we directly take the TF-IDF vectorizer.\n",
        "\n",
        " Without filtering, this produces very big vectors (e.g. the full concatenation corresponds to 1366006 dimensions!). \n",
        " Here, we choose to keep 5000 features, more than previously to take into account the new features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H93BYAfjdOcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7290bf69-c2f6-467c-a6fe-e8b47c245240"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TEST BI-GRAMS\n",
        "vectorizer = TfidfVectorizer( analyzer = 'word', max_features = 5000, ngram_range=(2,2) )\n",
        "train_features_tfidf_ngram = vectorizer.fit_transform( train_reviews )\n",
        "test_features_tfidf_ngram = vectorizer.transform( test_reviews )\n",
        "\n",
        "print( train_features_tfidf_ngram.shape, test_features_tfidf_ngram.shape)\n",
        "\n",
        "# Training\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit( train_features_tfidf_ngram, train_labels )\n",
        "# Predictions\n",
        "preds = classifier.predict( test_features_tfidf_ngram )\n",
        "# Scores\n",
        "print( classification_report( test_labels, preds ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 5000) (500, 5000)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       250\n",
            "           1       0.86      0.81      0.83       250\n",
            "\n",
            "    accuracy                           0.84       500\n",
            "   macro avg       0.84      0.84      0.84       500\n",
            "weighted avg       0.84      0.84      0.84       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr0LOc9vy_Hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64799a50-7d0a-448d-e389-99416b1b4fa9"
      },
      "source": [
        "# TEST TRI-GRAMS\n",
        "\n",
        "vectorizer = TfidfVectorizer( analyzer = 'word', max_features = 5000, ngram_range=(3,3) )\n",
        "train_features_tfidf_ngram = vectorizer.fit_transform( train_reviews )\n",
        "test_features_tfidf_ngram = vectorizer.transform( test_reviews )\n",
        "\n",
        "print( train_features_tfidf_ngram.shape, test_features_tfidf_ngram.shape)\n",
        "\n",
        "# Training\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit( train_features_tfidf_ngram, train_labels )\n",
        "# Predictions\n",
        "preds = classifier.predict( test_features_tfidf_ngram )\n",
        "# Scores\n",
        "print( classification_report( test_labels, preds ) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 5000) (500, 5000)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.75      0.75       250\n",
            "           1       0.75      0.75      0.75       250\n",
            "\n",
            "    accuracy                           0.75       500\n",
            "   macro avg       0.75      0.75      0.75       500\n",
            "weighted avg       0.75      0.75      0.75       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7KjJZYEzFYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b549a4-550b-48b5-d35a-754f319d49b5"
      },
      "source": [
        "# TEST UNIGRAMS + BI-GRAMS + TRI-GRAMS\n",
        "\n",
        "# TEST BI-GRAMS\n",
        "vectorizer = TfidfVectorizer( analyzer = 'word', max_features = 5000, ngram_range=(1,3) )\n",
        "train_features_tfidf_ngram = vectorizer.fit_transform( train_reviews )\n",
        "test_features_tfidf_ngram = vectorizer.transform( test_reviews )\n",
        "\n",
        "print( train_features_tfidf_ngram.shape, test_features_tfidf_ngram.shape)\n",
        "\n",
        "# Training\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit( train_features_tfidf_ngram, train_labels )\n",
        "# Predictions\n",
        "preds = classifier.predict( test_features_tfidf_ngram )\n",
        "# Scores\n",
        "print( classification_report( test_labels, preds ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 5000) (500, 5000)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.90      0.88       250\n",
            "           1       0.89      0.85      0.87       250\n",
            "\n",
            "    accuracy                           0.87       500\n",
            "   macro avg       0.87      0.87      0.87       500\n",
            "weighted avg       0.87      0.87      0.87       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXjmWOd9nL5H"
      },
      "source": [
        "### 2.5.2 Finding the best model\n",
        "\n",
        "Usually, we will want to try out different parameters, in order to see what works best for our task. As such, we might experiment with:\n",
        "- Different features\n",
        "- Different classification algorithms \n",
        "- Different model parameters\n",
        "\n",
        "However, we have to be careful: we cannot use our test set over and over again, as we’ll be optimizing our parameters for that particular test set, and run the risk of overfitting, which means we are not able to properly generalize to data we haven’t trained on.\n",
        "We want to build a model that is robust, meaning that it will get good performance on unseen data.\n",
        "That's why we only use the test set at the end, with the best model. \n",
        "\n",
        "For this reason, we need to make use of a validation our development set. \n",
        "However, our training set is already quite small; creating a separate validation set would give us even less training data. \n",
        "\n",
        "Fortunately, there is another option: we can use k-fold cross validation. \n",
        "The idea is the following:\n",
        "- Break up data into k (e.g. 10) parts (folds) \n",
        "- For each fold\n",
        "    - Current fold is used as temporary test set \n",
        "    - Use other 9 folds as training data\n",
        "    - Performance is computed on test fold\n",
        "- Average performance over 10 runs\n",
        "\n",
        "Scikit provides efficient ways of performing cross-fold validation.\n",
        "We will test below the grid search that allows to choose the best values for the hyper-parameters, using cross-validation over the trianing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EP2EnB1XOHF"
      },
      "source": [
        "#### 2.5.2.1 Optimizing the hyper-parameters\n",
        "\n",
        "Each algorithm comes with some \"options\" called hyper-parameters.\n",
        "The chosen values can have an important effect on the results.  \n",
        "\n",
        "For example, Logistic Regression has:\n",
        "- 'C' a coefficient C used for regularization, with smaller values specifying stronger regularization. \n",
        "- 'max_iter' (default=100) Maximum number of iterations taken for the solvers to converge.\n",
        "\n",
        "Here we use the class 'GridSearchCV' that will perform an exhaustive search over specified parameter values for an estimator (i.e. a classifier).\n",
        "We specify the algorithm we want (here 'LogisticRegression') and the parameters values we want to test (see the dictionnary 'parameters').\n",
        "\n",
        "Then the 'fit' method over the GridSearchCV object allows to perform the search over the parameters, using a cross-fold validation (default: 5-fold CV).\n",
        "\n",
        "Then, you can print the best set of parameters and the best score (i.e. Mean cross-validated score of the best_estimator), and use a panda dataframe to visualize the results according to each set of parameters.\n",
        "\n",
        "\n",
        "See the doc: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUj_rkBmXR6t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "359825c3-7434-4896-d1c5-b3be462f67df"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'C':[0.01, 0.1, 0.5, 1, 10, 50], 'max_iter':[50, 100] }\n",
        "lr = LogisticRegression()\n",
        "clf_lr = GridSearchCV(lr, parameters, verbose=1)\n",
        "clf_lr.fit( train_features_tfidf_ngram, train_labels )\n",
        "sorted(clf_lr.cv_results_.keys())\n",
        "\n",
        "print( \"Best parameters found:\", clf_lr.best_params_)\n",
        "print( \"Best score found:\", clf_lr.best_score_)\n",
        "\n",
        "pd.concat([pd.DataFrame(clf_lr.cv_results_[\"params\"]),pd.DataFrame(clf_lr.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best parameters found: {'C': 10, 'max_iter': 50}\n",
            "Best score found: 0.859\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        C  max_iter  Accuracy\n",
              "0    0.01        50    0.7804\n",
              "1    0.01       100    0.7804\n",
              "2    0.10        50    0.8138\n",
              "3    0.10       100    0.8138\n",
              "4    0.50        50    0.8456\n",
              "5    0.50       100    0.8456\n",
              "6    1.00        50    0.8542\n",
              "7    1.00       100    0.8542\n",
              "8   10.00        50    0.8590\n",
              "9   10.00       100    0.8590\n",
              "10  50.00        50    0.8508\n",
              "11  50.00       100    0.8506"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a2c963ea-67b6-4e1f-8724-dd0d08708dfe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>C</th>\n",
              "      <th>max_iter</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>50</td>\n",
              "      <td>0.7804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01</td>\n",
              "      <td>100</td>\n",
              "      <td>0.7804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.10</td>\n",
              "      <td>50</td>\n",
              "      <td>0.8138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.10</td>\n",
              "      <td>100</td>\n",
              "      <td>0.8138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.50</td>\n",
              "      <td>50</td>\n",
              "      <td>0.8456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.50</td>\n",
              "      <td>100</td>\n",
              "      <td>0.8456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.00</td>\n",
              "      <td>50</td>\n",
              "      <td>0.8542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.00</td>\n",
              "      <td>100</td>\n",
              "      <td>0.8542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10.00</td>\n",
              "      <td>50</td>\n",
              "      <td>0.8590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10.00</td>\n",
              "      <td>100</td>\n",
              "      <td>0.8590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>50.00</td>\n",
              "      <td>50</td>\n",
              "      <td>0.8508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>50.00</td>\n",
              "      <td>100</td>\n",
              "      <td>0.8506</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2c963ea-67b6-4e1f-8724-dd0d08708dfe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a2c963ea-67b6-4e1f-8724-dd0d08708dfe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a2c963ea-67b6-4e1f-8724-dd0d08708dfe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v2gFaikpc3h"
      },
      "source": [
        "You can then directly use the GridSearchCV object (here called 'clf') to make predictions on your test set: it correspond to the best model found during the search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il2DEVOpZuNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fffd91-c09d-42a3-94f6-a2d160c1f0a9"
      },
      "source": [
        "preds = clf_lr.predict( test_features_tfidf_ngram )\n",
        "print( classification_report( test_labels, preds ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       250\n",
            "           1       0.90      0.83      0.86       250\n",
            "\n",
            "    accuracy                           0.87       500\n",
            "   macro avg       0.87      0.87      0.87       500\n",
            "weighted avg       0.87      0.87      0.87       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypnlIloLxjFO"
      },
      "source": [
        "#### 2.5.2.2 Try other algorithms\n",
        "\n",
        "Now, you can use a similar code to test other algorithms (e.g. Naive Bayes, SVM). \n",
        "You only need to perform the grid search, we should only report the results on the test set for the best algorithm.\n",
        "\n",
        "Doc for Naive Bayes: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n",
        "\n",
        "Doc for SVM: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
        "\n",
        "Which one performs the best?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7h50aQdD8c9"
      },
      "source": [
        "# Testing Naive Bayes\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "parameters = {'alpha':[0, 0.1, 0.5, 0.8, 1] }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQPGtmtfFSy2"
      },
      "source": [
        "# Testing SVM\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# here, default max_iter = 1000\n",
        "parameters = {'C':[0.01, 0.1, 0.5, 1, 10, 50], 'max_iter':[100, 1000] }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeVgZiOBd7pB"
      },
      "source": [
        "## 2.6 Inspecting the model\n",
        "\n",
        "The linear classifiers work by learning weights over the features.\n",
        "Looking at these weights can give some insights on your model.\n",
        "\n",
        "With LogisticRegression in the binary setting, we have:\n",
        "- the most positive weights are the best indicators of the positive class (here positive reviews)\n",
        "- the most negative weights are the best indicators of the negative class (here negative reviews)\n",
        "\n",
        "The code below will print the 50 most positive and negative features: do the results make sense? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfSxjmu6d9tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d1752f-1544-469f-8fae-ef0b217b6b1e"
      },
      "source": [
        "# Here we look at the best model obtained with grid search, ngrams features and tf idf normalization\n",
        "\n",
        "vocab = vectorizer.get_feature_names()\n",
        "allCoefficients = [(clf_lr.best_estimator_.coef_[0,i], vocab[i]) for i in range(len(vocab))]\n",
        "allCoefficients.sort()\n",
        "allCoefficients.reverse()\n",
        "\n",
        "print(\"Top features for positive class:\")\n",
        "print( '\\n'.join( [ f+':\\t'+str((round(w,3))) for (w,f) in allCoefficients[:50]] ) )\n",
        "\n",
        "print(\"\\nTop features for negative class:\")\n",
        "print( '\\n'.join( [ f+':'+str((round(w,3))) for (w,f) in allCoefficients[-50:]] ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top features for positive class:\n",
            "great:\t8.101\n",
            "perfect:\t6.001\n",
            "fun:\t5.742\n",
            "excellent:\t5.311\n",
            "this is:\t4.991\n",
            "simple:\t4.704\n",
            "best:\t4.499\n",
            "creative:\t4.386\n",
            "wonderful:\t4.377\n",
            "heart:\t4.372\n",
            "gem:\t4.357\n",
            "enjoy:\t4.279\n",
            "masterpiece:\t4.218\n",
            "beautiful:\t4.215\n",
            "loved:\t4.193\n",
            "perfectly:\t4.147\n",
            "enjoyable:\t4.064\n",
            "is great:\t3.996\n",
            "him:\t3.95\n",
            "dream:\t3.941\n",
            "and:\t3.896\n",
            "incredible:\t3.882\n",
            "awesome:\t3.876\n",
            "job:\t3.748\n",
            "was great:\t3.722\n",
            "each:\t3.693\n",
            "family:\t3.655\n",
            "amazing:\t3.653\n",
            "entertaining:\t3.588\n",
            "well:\t3.58\n",
            "others:\t3.575\n",
            "subtle:\t3.569\n",
            "one of:\t3.569\n",
            "the best:\t3.548\n",
            "noir:\t3.546\n",
            "both:\t3.533\n",
            "highly:\t3.452\n",
            "enjoyed this:\t3.431\n",
            "surprised:\t3.413\n",
            "powerful:\t3.377\n",
            "still:\t3.348\n",
            "fantastic:\t3.347\n",
            "brilliant:\t3.338\n",
            "genre:\t3.319\n",
            "underrated:\t3.315\n",
            "first time:\t3.31\n",
            "necessary:\t3.292\n",
            "enjoyed:\t3.282\n",
            "today:\t3.276\n",
            "superb:\t3.244\n",
            "\n",
            "Top features for negative class:\n",
            "seems:-3.332\n",
            "apparently:-3.336\n",
            "pathetic:-3.353\n",
            "the only:-3.367\n",
            "wrote:-3.372\n",
            "zombies:-3.383\n",
            "mess:-3.411\n",
            "dreadful:-3.454\n",
            "than this:-3.524\n",
            "unfortunately:-3.551\n",
            "unless:-3.626\n",
            "oh:-3.631\n",
            "any:-3.716\n",
            "not worth:-3.761\n",
            "the good:-3.762\n",
            "pointless:-3.763\n",
            "terrible:-3.838\n",
            "ridiculous:-3.868\n",
            "disappointed:-3.937\n",
            "only:-3.953\n",
            "of the worst:-3.972\n",
            "no:-3.987\n",
            "instead:-3.999\n",
            "garbage:-4.046\n",
            "lame:-4.1\n",
            "laughable:-4.176\n",
            "at all:-4.278\n",
            "script:-4.361\n",
            "wonder:-4.423\n",
            "badly:-4.578\n",
            "even:-4.582\n",
            "predictable:-4.623\n",
            "boring:-4.646\n",
            "wasted:-4.951\n",
            "cheap:-4.99\n",
            "horrible:-5.07\n",
            "poorly:-5.328\n",
            "disappointment:-5.455\n",
            "nothing:-5.596\n",
            "disappointing:-5.747\n",
            "waste:-5.802\n",
            "annoying:-5.884\n",
            "worse:-6.0\n",
            "stupid:-6.53\n",
            "the worst:-6.589\n",
            "dull:-6.847\n",
            "awful:-7.766\n",
            "poor:-7.988\n",
            "worst:-8.642\n",
            "bad:-9.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It2say-ofOOn"
      },
      "source": [
        "# Part 3: generating word embeddings\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1eLkKWp8yOP6AJsK2h6Btbr3L95TvDsyD)\n",
        "\n",
        "\n",
        "\n",
        "As introduced during the course, we can use neural networks to generate vectors representing words.\n",
        "These vectors, learned on massive amount of data, allow to compute similarity measures between words.\n",
        "\n",
        "As an introductive exercise, we will generate word embeddings from the sentiment review dataset and take a look at the generated vectors.\n",
        "\n",
        "Remind that this corpus is \"small\", compared to what is generally used for generating embeddings, here around 40k words against millions of words in general! \n",
        "The resulting vectors will thus not be of extremely good quality (but the model will run very fast :). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cANCUNNJf_oM"
      },
      "source": [
        "## 3.1 Generating word embeddings\n",
        "\n",
        "We  will  use gensim in  order  to  induce  word  embeddings  from  text.\n",
        "gensim is  a  vector  space modeling and topic modeling toolkit for python, and contains an efficient implementation of the word2vec algorithms.\n",
        "\n",
        "word2vec consists of two different algorithms: skipgram (sg) and continuous-bag-of-words (cbow). \n",
        "The underlying prediction task of the former is to estimate the context words from the target word ; the prediction task of the latter is to estimate the target word from the sum of the context words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSgCIkdRfRsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5306457f-cf23-4e81-bf47-56d94fe43098"
      },
      "source": [
        "from gensim.models import Word2Vec \n",
        "\n",
        "import gzip\n",
        "import logging\n",
        "\n",
        "import time\n",
        "\n",
        "# set up logging for gensim\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "                    level=logging.INFO)\n",
        "\n",
        "# we define a PlainTextCorpus class; this will provide us with an\n",
        "# iterator over the corpus (so that we don't have to load the corpus\n",
        "# into memory)\n",
        "class PlainTextCorpus(object):\n",
        "    def __init__(self, fileName):\n",
        "        self.fileName = fileName\n",
        "\n",
        "    def __iter__(self):\n",
        "        for line in gzip.open(self.fileName, 'rt', encoding='utf-8'):\n",
        "            yield  line.split()\n",
        "\n",
        "# instantiate the corpus class using corpus location\n",
        "sentences = PlainTextCorpus('raw_reviews.txt.gz')\n",
        "\n",
        "# we only take into account words with a frequency of at least 50, and\n",
        "# we iterate over the corpus only once\n",
        "model = Word2Vec(sentences, min_count=50, iter=1)\n",
        "\n",
        "# finally, save the constructed model to disk\n",
        "model.save('model_word2vec')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-14 16:57:30,708 : INFO : collecting all words and their counts\n",
            "2022-04-14 16:57:30,713 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2022-04-14 16:57:31,932 : INFO : PROGRESS: at sentence #10000, processed 2354780 words, keeping 163178 word types\n",
            "2022-04-14 16:57:33,195 : INFO : PROGRESS: at sentence #20000, processed 4686268 words, keeping 251892 word types\n",
            "2022-04-14 16:57:33,785 : INFO : collected 289705 word types from a corpus of 5844706 raw words and 25000 sentences\n",
            "2022-04-14 16:57:33,787 : INFO : Loading a fresh vocabulary\n",
            "2022-04-14 16:57:33,942 : INFO : effective_min_count=50 retains 7900 unique words (2% of original 289705, drops 281805)\n",
            "2022-04-14 16:57:33,943 : INFO : effective_min_count=50 leaves 4935387 word corpus (84% of original 5844706, drops 909319)\n",
            "2022-04-14 16:57:33,976 : INFO : deleting the raw counts dictionary of 289705 items\n",
            "2022-04-14 16:57:33,985 : INFO : sample=0.001 downsamples 47 most-common words\n",
            "2022-04-14 16:57:33,987 : INFO : downsampling leaves estimated 3629505 word corpus (73.5% of prior 4935387)\n",
            "2022-04-14 16:57:34,035 : INFO : estimated required memory for 7900 words and 100 dimensions: 10270000 bytes\n",
            "2022-04-14 16:57:34,036 : INFO : resetting layer weights\n",
            "2022-04-14 16:57:35,561 : INFO : training model with 3 workers on 7900 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
            "2022-04-14 16:57:36,575 : INFO : EPOCH 1 - PROGRESS: at 11.39% examples, 410960 words/s, in_qsize 5, out_qsize 0\n",
            "2022-04-14 16:57:37,596 : INFO : EPOCH 1 - PROGRESS: at 23.55% examples, 425667 words/s, in_qsize 5, out_qsize 0\n",
            "2022-04-14 16:57:38,618 : INFO : EPOCH 1 - PROGRESS: at 35.02% examples, 421115 words/s, in_qsize 5, out_qsize 0\n",
            "2022-04-14 16:57:39,623 : INFO : EPOCH 1 - PROGRESS: at 47.24% examples, 425090 words/s, in_qsize 5, out_qsize 0\n",
            "2022-04-14 16:57:40,626 : INFO : EPOCH 1 - PROGRESS: at 58.42% examples, 422607 words/s, in_qsize 5, out_qsize 0\n",
            "2022-04-14 16:57:41,631 : INFO : EPOCH 1 - PROGRESS: at 70.63% examples, 425168 words/s, in_qsize 6, out_qsize 0\n",
            "2022-04-14 16:57:42,637 : INFO : EPOCH 1 - PROGRESS: at 83.00% examples, 427529 words/s, in_qsize 5, out_qsize 0\n",
            "2022-04-14 16:57:43,664 : INFO : EPOCH 1 - PROGRESS: at 95.27% examples, 427919 words/s, in_qsize 5, out_qsize 1\n",
            "2022-04-14 16:57:43,998 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2022-04-14 16:57:44,000 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-04-14 16:57:44,018 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-04-14 16:57:44,020 : INFO : EPOCH - 1 : training on 5844706 raw words (3628802 effective words) took 8.5s, 429217 effective words/s\n",
            "2022-04-14 16:57:44,022 : INFO : training on a 5844706 raw words (3628802 effective words) took 8.5s, 428956 effective words/s\n",
            "2022-04-14 16:57:44,025 : INFO : saving Word2Vec object under model_word2vec, separately None\n",
            "2022-04-14 16:57:44,027 : INFO : not storing attribute vectors_norm\n",
            "2022-04-14 16:57:44,031 : INFO : not storing attribute cum_table\n",
            "2022-04-14 16:57:44,136 : INFO : saved model_word2vec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7CNItSggh2P"
      },
      "source": [
        "## 3.2 Compute word similarity\n",
        "\n",
        "You can now compute the most similar words (which is measured by cosine similarity between the word vectors) by issuing the following command:\n",
        "\n",
        "model.wv.most_similar(myword)\n",
        "\n",
        "Don't hesitate to test with other words, such as \"movie\", \"good\" etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGUQeqZxgrne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92483021-2e1c-455b-bc86-6bed07c2bc6e"
      },
      "source": [
        "model.wv.most_similar('actor')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-14 16:58:43,104 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('actress', 0.9464222192764282),\n",
              " ('performance', 0.8889339566230774),\n",
              " ('role', 0.8721189498901367),\n",
              " ('actor,', 0.837760329246521),\n",
              " ('role,', 0.8222366571426392),\n",
              " ('character,', 0.8029254674911499),\n",
              " ('role.', 0.7964649200439453),\n",
              " ('actor.', 0.7946804761886597),\n",
              " ('character', 0.7902258634567261),\n",
              " ('actress.', 0.7811508774757385)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiAMuEhy09Dc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2ewU_ig09O4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2MAHA5ug6mk"
      },
      "source": [
        "Word  embeddings  allow  us  to  do  analogical  reasoning  using  vector  addition and subtraction. \n",
        "gensim offers the possibility to do so. \n",
        "\n",
        "Try to perform analogical reasoning,  e.g.  actor - man  +  woman  = ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6IMcawVhP0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646b562f-7ef8-4384-f47d-664be27322eb"
      },
      "source": [
        "model.most_similar(positive=[\"actor\", \"woman\"], negative=[\"man\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('actress', 0.948756754398346),\n",
              " ('performance', 0.894486129283905),\n",
              " ('role', 0.8632146120071411),\n",
              " ('actor,', 0.8322696685791016),\n",
              " ('role,', 0.8218492865562439),\n",
              " ('actress,', 0.8159606456756592),\n",
              " ('character,', 0.8123709559440613),\n",
              " ('voice', 0.8086908459663391),\n",
              " ('actress.', 0.8020336627960205),\n",
              " ('actor.', 0.7969970703125)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8-b2PschV81"
      },
      "source": [
        "## 3.3 Modify the model\n",
        "\n",
        "As a default, the word2vec module creates word embeddings with the following setting:\n",
        "- algorithm: CBOW\n",
        "- window: 5 \n",
        "- embeddings size: 100\n",
        "\n",
        "Try other options, including:\n",
        "- algorithm: skipgram\n",
        "- window: try varied sizes, from very small to large one\n",
        "- embeddings size: try varied sizes, from very small to large one\n",
        "\n",
        "Each time, evaluate the impact on the similarity computation.\n",
        "What configuration works best?\n",
        "\n",
        "See doc: https://radimrehurek.com/gensim_3.8.3/models/word2vec.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r_3PC-oiu-d"
      },
      "source": [
        "# a- MODIFYING THE WINDOW SIZE (here 1)\n",
        "\n",
        "model_w1 = Word2Vec(sentences, min_count=50, iter=1, window=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRXKwejKjuQ-"
      },
      "source": [
        "# a- MODIFYING THE WINDOW SIZE (e.g. 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PurMhdRSi7OD"
      },
      "source": [
        "# b- MODIFYING THE EMBEDDINGS SIZE (here 10)\n",
        "\n",
        "model_s10 = Word2Vec(sentences, min_count=50, iter=1, size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKu6Ot6tjRFe"
      },
      "source": [
        "# b- MODIFYING THE EMBEDDINGS SIZE (e.g. 300)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGAe2UXaiB8z"
      },
      "source": [
        "# c- WITH SKIPGRAM\n",
        "\n",
        "# we only take into account words with a frequency of at least 50, and\n",
        "# we iterate over the corpus only once\n",
        "model_sg = Word2Vec(sentences, min_count=50, iter=1, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGHlQwrK1lCf"
      },
      "source": [
        "model.wv.most_similar('actor')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq76HGzqiN9b"
      },
      "source": [
        "### Note\n",
        "\n",
        "According to Mikolov:\n",
        "- Skip-gram: works well with small amount of the training data, represents well even rare words or phrases.\n",
        "- CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZintyoifSER"
      },
      "source": [
        "# Part 4 (Advanced): neural based classification\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1rFv3huUMTbtEHepb8vlluA61H107ow9R)\n",
        "\n",
        "Use a neural network to perform sentiment analysis using:\n",
        "- randomly initialized word representations\n",
        "- the embeddings built with Gensim\n",
        "- pre-trained word embeddings (e.g. FastText, Glove etc)\n",
        "\n",
        "A solution based on TensorFlow will be provided later on the course website)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGNYidpBBkGr"
      },
      "source": [
        "# Additional notes: coreference resolution\n",
        "\n",
        "You can also use a model called neuralCoref to perform Coreference resolution.\n",
        "However, it requires an older version of Spacy (https://github.com/huggingface/neuralcoref/issues/207). Below, there is a way to make it work within the notebook (if you don't use a notebook, install a virtual environement with conda to work with Spacy 2.1).\n",
        "\n",
        "You can try it at home, but for now, just take a look at the last cell to inspect the results (presented in the course slides).\n",
        "\n",
        "\n",
        "Using neuralcoref:\n",
        "* spacy: https://spacy.io/universe/project/neuralcoref-vizualizer \n",
        "* git page: https://github.com/huggingface/neuralcoref\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_eT1v-GBwbm"
      },
      "source": [
        "pip install -U spacy==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUR2J3NEBmtu"
      },
      "source": [
        "pip install neuralcoref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkFKiFkMfqWt"
      },
      "source": [
        "# Dowload a model within Google colab: https://stackoverflow.com/questions/49259404/how-to-install-models-download-packages-on-google-colab\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j99Td8buCISy"
      },
      "source": [
        "# Load your usual SpaCy model (one of SpaCy English models)\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# load NeuralCoref and add it to the pipe of SpaCy's model\n",
        "import neuralcoref\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFfy0WE7CM-Q"
      },
      "source": [
        "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
        "nlp.add_pipe(coref, name='neuralcoref')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7SwfY8mCQ_B"
      },
      "source": [
        "# You're done. You can now use NeuralCoref the same way you usually manipulate a SpaCy document and it's annotations.\n",
        "doc = nlp(u'Nina gave Tom the burger. He was hungry. He ate it.')\n",
        "doc._.has_coref\n",
        "doc._.coref_clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgkr7ugLCWu2"
      },
      "source": [
        "### Note on coreference\n",
        "\n",
        "In the previous cell, you can see the results of a coreference system over three sentences.\n",
        "The system finds two clusters:\n",
        "- one corresponding to 'Tom', containing the proper noun 'Tom', and two occurences of the pronoun 'He'.\n",
        "- one corresponding to 'the burger', containing the noun 'the burger' and one occurence of the pronoun 'it'\n",
        "\n",
        "Here, the system doesn't identify 'singleton', meaning mention with only one reference ('Tina').\n",
        "\n",
        "See below what happens with a slight change in the sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU96sE9kCbT8"
      },
      "source": [
        "doc = nlp(u'Nina gave Tom the burger. She was hungry. He ate it.')\n",
        "doc._.has_coref\n",
        "doc._.\n",
        "\n",
        "# If we add a mention to 'Nina' using the corresponding pronoun 'She', then we have three clusters, one for each entity. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-MywoYCzpJ9"
      },
      "source": [
        "# Additional notes: NLTK\n",
        "\n",
        "Examples of pre-processing using NLTK, another great library for NLP (though a bit older)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjk5gf5jkzUu"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCI-KtEMmawF"
      },
      "source": [
        "text = \"This ice-cream is from the U.K.. This isn't a cake. \"\n",
        "sentences = nltk.sent_tokenize(text) # Sentence splitting\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY9BzkUUkcpG"
      },
      "source": [
        "sentence = 'She sells seashells on the seashore.'\n",
        "tokens = nltk.word_tokenize(sentence) # Tokenization\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiAYyiRomjGu"
      },
      "source": [
        "text = \"This ice-cream is from the U.K.. This isn't a cake. \"\n",
        "for sentence in nltk.sent_tokenize(text):\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "  tagged_tokens = nltk.pos_tag(tokens) # POS tagging\n",
        "  print(tagged_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMVErk_Lmqy_"
      },
      "source": [
        "sentence = \"Time flies like an arrow.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tagged_tokens = nltk.pos_tag(tokens) # POS tagging\n",
        "print(tagged_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPCwgxzBo70t"
      },
      "source": [
        "# Lemmatizing with NLTK \n",
        "sentence = \"Mr. Dursley was the director of a firm called Grunnings, which made drills.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "verb_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
        "verbs = []\n",
        "for token, tag in tagged_tokens:\n",
        "    if tag in verb_tags:\n",
        "        verbs.append(token)\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "verb_lemmas = []\n",
        "for word_form in verbs:\n",
        "    lemma = lemmatizer.lemmatize(word_form, \"v\") \n",
        "    verb_lemmas.append((word_form,lemma))\n",
        "print( verb_lemmas )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbWEhmbKwpcB"
      },
      "source": [
        "Parsing with NLTK, a bit more complicated.\n",
        "\n",
        "See: \n",
        "- bllipparser: https://pypi.org/project/bllipparser/\n",
        "- Stanford parser: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtCbtiD1rXYw"
      },
      "source": [
        "pip install --user bllipparser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3r_8yCsx8VE"
      },
      "source": [
        "python3 -m nltk.downloader bllip_wsj_no_aux"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I8oHOOxwkJH"
      },
      "source": [
        "from bllipparser import RerankingParser\n",
        "\n",
        "rrp = RerankingParser.fetch_and_load('WSJ-PTB3', verbose=True)\n",
        "rrp.simple_parse(\"It's that easy.\")\n",
        "\n",
        "\n",
        "model_dir = find('models/bllip_wsj_no_aux').path\n",
        "parser = RerankingParser.from_unified_model_dir(model_dir)\n",
        "best = parser.parse(\"The old oak tree from India fell down.\")\n",
        "\n",
        "print(best.get_reranker_best())\n",
        "print(best.get_parser_best())\n",
        "\n",
        "from IPython.display import display\n",
        "display(resultparse_trees[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}